{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-_JtARm3tv6",
        "outputId": "a057934e-4b1a-4b05-d903-10b892f6e4a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Oct 23 08:45:30 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n",
            "| N/A   30C    P0             50W /  400W |       5MiB /  81920MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m36.2/36.2 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m144.1/144.1 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m828.5/828.5 kB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.0/188.0 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m832.4/832.4 kB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m176.0/176.0 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hâœ… æ‰€æœ‰ä¾èµ–å®‰è£…å®Œæˆ\n"
          ]
        }
      ],
      "source": [
        "#section 0\n",
        "# æŸ¥çœ‹ GPU\n",
        "!nvidia-smi\n",
        "\n",
        "# å®‰è£…ä¾èµ– - ä¿®æ­£ç‰ˆ\n",
        "!pip -q install \"transformers>=4.41\" accelerate datasets scikit-learn openpyxl\n",
        "\n",
        "# âœ… ä¿®æ­£ï¼šæ­£ç¡®å®‰è£… RDKit\n",
        "!pip -q install rdkit\n",
        "\n",
        "# âœ… ä¿®æ­£ï¼šæ­£ç¡®å®‰è£… Chemprop (æ³¨æ„ç‰ˆæœ¬)\n",
        "!pip -q install chemprop\n",
        "\n",
        "# å®‰è£… PyTorch (å¦‚æœéœ€è¦)\n",
        "!pip -q install torch torchvision torchaudio\n",
        "\n",
        "!pip install -q rdkit transformers accelerate datasets scikit-learn openpyxl torch bitsandbytes peft trl sentencepiece protobuf\n",
        "\n",
        "print(\"âœ… æ‰€æœ‰ä¾èµ–å®‰è£…å®Œæˆ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R5V-Y0wO3wl_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34,
          "referenced_widgets": [
            "2c75135ee25e4f6fa6358367d882a1ae",
            "4858747f49fc4bb9a6853200aeb92029",
            "4c8cbaa72e134b36b6e1659ec170005b",
            "226c574474884f6fb76c9646b5891142",
            "6438ac8f67fe45fbb0a2f726f030c161",
            "7b5ce8a38c374e7e984b703159b95acd",
            "295dceb1342a4d779fec1a15099d642d",
            "febff21a75964dae82d280a11d476ba4",
            "ca418843d9e04cdd95dc3a0287331ca2",
            "f4e05deca27041649e5dd2ff86db5793",
            "7039b5defbf940eb841d0dcd9bff7804",
            "ab9a4e85c32c48b891786408082e57d8",
            "6000573289ec44608924259cc3145544",
            "97b4a21c28164860bcc34ea3256b8ea4",
            "60585ca2b5a94e56a79981fe04ec05d1",
            "7b081dd3e63d45e393fbee38702a7e71",
            "2a2b0a6cbf14450e9a978e164d0da1a1",
            "2f00fc252101471b8e50df78dfe351be",
            "fff8707fda3a464d8ef0d3f7ba0346f7",
            "f103bc5dcf214fa5b2a30d0ac1762bf3"
          ]
        },
        "outputId": "dc7c3d5e-9916-40d6-f67b-3021db2a6c38"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c75135ee25e4f6fa6358367d882a1ae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#section 1\n",
        "from huggingface_hub import login\n",
        "login()  # æŒ‰æç¤ºç²˜è´´ä½ çš„HF Tokenï¼ˆSettings -> Access Tokensï¼‰\n",
        "\n",
        "# å¯é€‰ï¼šæŒ‚è½½ Drive ä¾¿äºè¯»å–æ•°æ®/ä¿å­˜ç»“æœ\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#section 1 (åœ¨æŒ‚è½½ Drive ä¹‹å‰æˆ–ä¹‹å)\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "# ä¸Šä¼ æ–‡ä»¶\n",
        "print(\"ğŸ“¤ è¯·é€‰æ‹©æ‚¨çš„æ•°æ®æ–‡ä»¶...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# è·å–ä¸Šä¼ çš„æ–‡ä»¶å\n",
        "filename = list(uploaded.keys())[0]\n",
        "print(f\"âœ… æ–‡ä»¶å·²ä¸Šä¼ : {filename}\")\n",
        "\n",
        "# ç§»åŠ¨åˆ°å·¥ä½œç›®å½•ï¼ˆå¯é€‰ï¼‰\n",
        "if filename != \"smiles-data.xlsx\":\n",
        "    shutil.move(filename, \"smiles-data.xlsx\")\n",
        "    print(\"âœ… æ–‡ä»¶å·²é‡å‘½åä¸º: smiles-data.xlsx\")\n",
        "\n",
        "# éªŒè¯æ–‡ä»¶\n",
        "import pandas as pd\n",
        "df = pd.read_excel(\"smiles-data.xlsx\")\n",
        "print(f\"\\nğŸ“Š æ•°æ®é¢„è§ˆ:\")\n",
        "print(f\"   è¡Œæ•°: {len(df)}\")\n",
        "print(f\"   åˆ—å: {df.columns.tolist()}\")\n",
        "print(f\"\\nå‰3è¡Œ:\")\n",
        "print(df.head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "FbWaRrlxALMs",
        "outputId": "e5f8ed80-0cf6-4799-acf8-1e9daec9a602"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¤ è¯·é€‰æ‹©æ‚¨çš„æ•°æ®æ–‡ä»¶...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9fee87bb-171c-4dc5-8af8-eaef79787f26\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9fee87bb-171c-4dc5-8af8-eaef79787f26\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving smiles-data.xlsx to smiles-data (1).xlsx\n",
            "âœ… æ–‡ä»¶å·²ä¸Šä¼ : smiles-data (1).xlsx\n",
            "âœ… æ–‡ä»¶å·²é‡å‘½åä¸º: smiles-data.xlsx\n",
            "\n",
            "ğŸ“Š æ•°æ®é¢„è§ˆ:\n",
            "   è¡Œæ•°: 1200\n",
            "   åˆ—å: ['number', 1, 2, 3, 'Structure', 'Score']\n",
            "\n",
            "å‰3è¡Œ:\n",
            "   number   1   2   3                                          Structure  \\\n",
            "0       1  A1  C1  B1  CCCCCCCCCCCCNC(=O)C(CCCCCOC(=O)CCCCCCCCCC)NCCN...   \n",
            "1       2  A1  C1  B2  CCCCCCCCCCCCNC(=O)C(CCCCCOC(=O)CCC(C)CCCCC)NCC...   \n",
            "2       3  A1  C1  B3  CCCCCCCCCCCCNC(=O)C(CCCCCOC(=O)CCCCCCCCC)NCCN(C)C   \n",
            "\n",
            "      Score  \n",
            "0  4.056689  \n",
            "1  3.381791  \n",
            "2  2.374907  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# å¼ºåˆ¶åˆ é™¤æ—§æ–‡ä»¶ï¼Œé‡æ–°åˆ›å»º\n",
        "!rm -f /content/train_primary_then_agents_verify_test.py\n",
        "\n",
        "# ç„¶åé‡æ–°è¿è¡Œ Section 2"
      ],
      "metadata": {
        "id": "kP118JFBEy-A"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrMyzxOK4FEj",
        "outputId": "2bfec3f2-9266-4644-e252-6d29b2b674e2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.1)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2-DuyqLb3ysV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0054977-9515-444c-ad8c-e141e193b9e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train_deepseek_simple.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile train_deepseek_simple.py\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "DeepSeek + MolFormer - ç»ˆæç®€åŒ–ç‰ˆ\n",
        "\"\"\"\n",
        "import os, json, argparse, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM,\n",
        "    Trainer, TrainingArguments, DataCollatorWithPadding, BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
        "import re\n",
        "\n",
        "# ========== è¾…åŠ©å‡½æ•° ==========\n",
        "def seed_everything(seed=42):\n",
        "    import random\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed); os.environ[\"PYTHONHASHSEED\"]=str(seed)\n",
        "    torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
        "\n",
        "def ensure_label_1_8(x): return max(1, min(8, int(round(float(x)))))\n",
        "\n",
        "def within_k(y_true, y_pred, k=1):\n",
        "    return float(np.mean(np.abs(np.array(y_true)-np.array(y_pred)) <= k))\n",
        "\n",
        "def quadratic_weighted_kappa(y_true, y_pred):\n",
        "    yt = np.array(y_true); yp = np.array(y_pred); m = 8\n",
        "    O = np.zeros((m,m))\n",
        "    for a,b in zip(yt,yp): O[a-1, b-1]+=1\n",
        "    W = np.array([[(i-j)**2/49 for j in range(m)] for i in range(m)])\n",
        "    act, pred = O.sum(1), O.sum(0)\n",
        "    E = np.outer(act, pred) / act.sum()\n",
        "    return 1.0 - (np.sum(W*O)/np.sum(W*E) if np.sum(W*E)>0 else 1.0)\n",
        "\n",
        "def evaluate_metrics(y_true, y_pred):\n",
        "    return {\"MAE\": float(mean_absolute_error(y_true, y_pred)),\n",
        "            \"withinÂ±1\": within_k(y_true, y_pred, 1),\n",
        "            \"QWK\": quadratic_weighted_kappa(y_true, y_pred)}\n",
        "\n",
        "# Scaffold split\n",
        "try:\n",
        "    from rdkit import Chem\n",
        "    from rdkit.Chem.Scaffolds import MurckoScaffold\n",
        "    def get_scaffold(s):\n",
        "        try: return Chem.MolToSmiles(MurckoScaffold.GetScaffoldForMol(Chem.MolFromSmiles(s)))\n",
        "        except: return None\n",
        "except:\n",
        "    def get_scaffold(s): return hash(s) % 10000\n",
        "\n",
        "def scaffold_split(df, smiles_col, test_size=120, val_frac=0.2, seed=42):\n",
        "    df = df.copy(); df[\"scaffold\"] = df[smiles_col].apply(get_scaffold)\n",
        "    df = df[df[\"scaffold\"].notna()].reset_index(drop=True)\n",
        "    groups = df.groupby(\"scaffold\").size().sort_values(ascending=False)\n",
        "\n",
        "    # âœ… ä¿®å¤ï¼šæ›´æ™ºèƒ½åœ°é€‰æ‹© scaffold\n",
        "    test_scaffolds, cnt = [], 0\n",
        "    for scaf, c in groups.items():\n",
        "        # å¦‚æœåŠ ä¸Šè¿™ä¸ª scaffold ä¸ä¼šè¶…å‡ºå¤ªå¤šï¼ˆå®¹å¿ 20% è¯¯å·®ï¼‰\n",
        "        if cnt + c <= test_size * 1.2:\n",
        "            test_scaffolds.append(scaf)\n",
        "            cnt += c\n",
        "            if cnt >= test_size:  # âœ… è¾¾åˆ°ç›®æ ‡å°±åœæ­¢\n",
        "                break\n",
        "        # å¦‚æœå½“å‰ä¸è¶³ï¼Œä¸”è¿™ä¸ª scaffold èƒ½å¡«è¡¥ä¸€åŠä»¥ä¸Šçš„ç¼ºå£\n",
        "        elif cnt < test_size and test_size - cnt > c * 0.5:\n",
        "            test_scaffolds.append(scaf)\n",
        "            cnt += c\n",
        "            break\n",
        "\n",
        "    test_df = df[df[\"scaffold\"].isin(test_scaffolds)]\n",
        "    remain_df = df[~df[\"scaffold\"].isin(test_scaffolds)]\n",
        "    train_df, val_df = train_test_split(remain_df, test_size=val_frac, random_state=seed)\n",
        "    print(f\"Split: Train={len(train_df)} Val={len(val_df)} Test={len(test_df)} (target={test_size})\")\n",
        "    return train_df.reset_index(drop=True), val_df.reset_index(drop=True), test_df.reset_index(drop=True)\n",
        "\n",
        "# ========== ğŸ”¥ ä½¿ç”¨åŸç”Ÿ Transformers Trainer (ä¸ç”¨ trl) ==========\n",
        "def train_llm_classifier(model_name, train_df, val_df, smiles_col, label_col, out_dir,\n",
        "                         epochs=5, lr=2e-4, bs=2, seed=42):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    seed_everything(seed)\n",
        "\n",
        "    print(f\"\\nğŸ”§ Loading DeepSeek: {model_name}\")\n",
        "\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_8bit=True, bnb_8bit_compute_dtype=torch.float16,\n",
        "        bnb_8bit_use_double_quant=True,\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    if not tokenizer.pad_token: tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name, quantization_config=bnb_config, device_map=\"auto\",\n",
        "        trust_remote_code=True, torch_dtype=torch.float16,\n",
        "    )\n",
        "\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "    lora_config = LoraConfig(\n",
        "        r=16, lora_alpha=32, target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
        "        lora_dropout=0.05, bias=\"none\", task_type=TaskType.CAUSAL_LM\n",
        "    )\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    # å‡†å¤‡æ•°æ®\n",
        "    def format_sample(row):\n",
        "        s = str(row[smiles_col]); score = ensure_label_1_8(row[label_col])\n",
        "        text = f\"Predict score (1-8) for molecule.\\nSMILES: {s}\\nScore: {score}\"\n",
        "        return tokenizer(text, truncation=True, max_length=256, padding=False)\n",
        "\n",
        "    train_data = [format_sample(row) for _, row in train_df.iterrows()]\n",
        "    val_data = [format_sample(row) for _, row in val_df.iterrows()]\n",
        "\n",
        "    # ğŸ”¥ ä½¿ç”¨åŸç”Ÿ Trainer (é¿å… trl å…¼å®¹æ€§é—®é¢˜)\n",
        "    from torch.utils.data import Dataset\n",
        "    class SimpleDataset(Dataset):\n",
        "        def __init__(self, data): self.data = data\n",
        "        def __len__(self): return len(self.data)\n",
        "        def __getitem__(self, i):\n",
        "            item = {k: torch.tensor(v) for k,v in self.data[i].items()}\n",
        "            item[\"labels\"] = item[\"input_ids\"].clone()\n",
        "            return item\n",
        "\n",
        "    train_ds = SimpleDataset(train_data)\n",
        "    val_ds = SimpleDataset(val_data)\n",
        "\n",
        "    def collate_fn(batch):\n",
        "        return {k: torch.nn.utils.rnn.pad_sequence(\n",
        "            [b[k] for b in batch], batch_first=True, padding_value=tokenizer.pad_token_id\n",
        "        ) for k in batch[0].keys()}\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=out_dir, num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=bs, per_device_eval_batch_size=bs,\n",
        "        gradient_accumulation_steps=8, learning_rate=lr, fp16=True,\n",
        "        logging_steps=20, eval_strategy=\"epoch\", save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True, warmup_ratio=0.1, report_to=\"none\",\n",
        "        save_total_limit=2, seed=seed,\n",
        "    )\n",
        "\n",
        "    from transformers import default_data_collator\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model, args=args, train_dataset=train_ds, eval_dataset=val_ds,\n",
        "        data_collator=collate_fn, tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    print(\"ğŸš€ Training DeepSeek (15-20h)...\")\n",
        "    trainer.train()\n",
        "    trainer.save_model(out_dir); tokenizer.save_pretrained(out_dir)\n",
        "    return model, tokenizer\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_llm(model, tokenizer, df, smiles_col, label_col):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.eval(); predictions = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        prompt = f\"Predict score (1-8) for molecule.\\nSMILES: {row[smiles_col]}\\nScore:\"\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=256)\n",
        "        inputs = {k:v.to(device) for k,v in inputs.items()}\n",
        "\n",
        "        try:\n",
        "            outputs = model.generate(**inputs, max_new_tokens=3, temperature=0.1, do_sample=False)\n",
        "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            match = re.search(r'Score:\\s*(\\d)', response)\n",
        "            score = int(match.group(1)) if match else 4\n",
        "            score = max(1, min(8, score))\n",
        "        except: score = 4\n",
        "\n",
        "        predictions.append(score)\n",
        "        if (idx+1)%30==0: print(f\"   {idx+1}/{len(df)}\")\n",
        "\n",
        "    return np.array(predictions), None\n",
        "\n",
        "# ========== MolFormer ==========\n",
        "class SmilesDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, tok, text_col, label_col):\n",
        "        self.df=df; self.tok=tok; self.text_col=text_col\n",
        "        self.labels = [ensure_label_1_8(x)-1 for x in df[label_col]]\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, i):\n",
        "        enc = self.tok(str(self.df.iloc[i][self.text_col]), truncation=True, max_length=256)\n",
        "        return {k:torch.tensor(v) for k,v in enc.items()} | {\"labels\": torch.tensor(self.labels[i])}\n",
        "\n",
        "# ä¿®å¤åçš„ train_molformer å‡½æ•°\n",
        "def train_molformer(model_name, train_df, val_df, smiles_col, label_col, out_dir,\n",
        "                    epochs=12, lr=1e-5, bs=8, seed=42):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    seed_everything(seed)\n",
        "    print(f\"\\nğŸ”§ Training {model_name}\")\n",
        "\n",
        "    tok = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name, num_labels=8, trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    train_ds = SmilesDataset(train_df, tok, smiles_col, label_col)\n",
        "    val_ds = SmilesDataset(val_df, tok, smiles_col, label_col)\n",
        "\n",
        "    # âœ… ä½¿ç”¨ default_data_collator è€Œä¸æ˜¯ DataCollatorWithPadding\n",
        "    from transformers import default_data_collator\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=out_dir,\n",
        "        seed=seed,\n",
        "        learning_rate=lr,\n",
        "        num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=bs,\n",
        "        per_device_eval_batch_size=bs,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        report_to=\"none\",\n",
        "        save_total_limit=2,\n",
        "        logging_steps=50,\n",
        "        warmup_ratio=0.1,\n",
        "        weight_decay=0.1,\n",
        "        dataloader_num_workers=0,  # âœ… é¿å…å¤šè¿›ç¨‹é—®é¢˜\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=val_ds,\n",
        "        data_collator=default_data_collator,  # âœ… ä¿®å¤å…³é”®\n",
        "        tokenizer=tok,\n",
        "    )\n",
        "\n",
        "    print(f\"   Training for {epochs} epochs...\")\n",
        "    trainer.train()\n",
        "    trainer.save_model(out_dir)\n",
        "    tok.save_pretrained(out_dir)\n",
        "    print(f\"   âœ… Saved to {out_dir}\")\n",
        "    return model, tok\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_molformer(model, tok, df, smiles_col, label_col):\n",
        "    ds = SmilesDataset(df, tok, smiles_col, label_col)\n",
        "    loader = torch.utils.data.DataLoader(ds, batch_size=64, collate_fn=DataCollatorWithPadding(tok))\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device).eval(); preds = []\n",
        "    for batch in loader:\n",
        "        _ = batch.pop(\"labels\")\n",
        "        logits = model(**{k:v.to(device) for k,v in batch.items()}).logits\n",
        "        preds.extend((logits.argmax(-1)+1).cpu().tolist())\n",
        "    return np.array(preds), None\n",
        "\n",
        "# ========== Main ==========\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--data_path\", default=\"/content/smiles-data.xlsx\")\n",
        "    ap.add_argument(\"--smiles_col\", default=\"Structure\")\n",
        "    ap.add_argument(\"--label_col\", default=\"Score\")\n",
        "    ap.add_argument(\"--out_dir\", default=\"./outputs_deepseek\")\n",
        "    ap.add_argument(\"--seed\", type=int, default=42)\n",
        "    ap.add_argument(\"--epochs_llm\", type=int, default=5)\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    seed_everything(args.seed)\n",
        "    os.makedirs(args.out_dir, exist_ok=True)\n",
        "\n",
        "    # åŠ è½½æ•°æ®\n",
        "    df = pd.read_excel(args.data_path)\n",
        "    df = df.dropna(subset=[args.smiles_col, args.label_col])\n",
        "    df[args.label_col] = df[args.label_col].map(ensure_label_1_8)\n",
        "\n",
        "    train_df, val_df, test_df = scaffold_split(df, args.smiles_col, 120, 0.2, args.seed)\n",
        "\n",
        "    # è®­ç»ƒ DeepSeek\n",
        "    print(\"\\n[Stage 1] DeepSeek PRIMARY\")\n",
        "    m_pri, t_pri = train_llm_classifier(\n",
        "        \"deepseek-ai/deepseek-llm-7b-base\", train_df, val_df,\n",
        "        args.smiles_col, args.label_col,\n",
        "        os.path.join(args.out_dir, \"primary\"),\n",
        "        epochs=args.epochs_llm, seed=args.seed\n",
        "    )\n",
        "\n",
        "    print(\"\\nâœ… Training complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_deepseek_simple.py \\\n",
        "    --data_path /content/smiles-data.xlsx \\\n",
        "    --epochs_llm 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-YKmJ4i9PiV",
        "outputId": "20db73f0-9f96-40ec-d6d1-dedc4c9e04f5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-10-23 08:49:33.754879: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761209373.776623    8196 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761209373.783248    8196 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761209373.799766    8196 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761209373.799793    8196 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761209373.799797    8196 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761209373.799800    8196 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Split: Train=816 Val=204 Test=180 (target=120)\n",
            "\n",
            "[Stage 1] DeepSeek PRIMARY\n",
            "\n",
            "ğŸ”§ Loading DeepSeek: deepseek-ai/deepseek-llm-7b-base\n",
            "tokenizer_config.json: 100% 792/792 [00:00<00:00, 5.18MB/s]\n",
            "tokenizer.json: 4.61MB [00:00, 47.9MB/s]\n",
            "config.json: 100% 584/584 [00:00<00:00, 4.88MB/s]\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "pytorch_model.bin.index.json: 22.5kB [00:00, 59.4MB/s]\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "pytorch_model-00002-of-00002.bin:   0% 0.00/3.85G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   0% 0.00/9.97G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model.safetensors.index.json: 23.6kB [00:00, 65.7MB/s]\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:   0% 707k/3.85G [00:01<2:54:14, 368kB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   0% 6.13M/9.97G [00:01<51:17, 3.24MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   0% 2.21M/3.85G [00:02<47:01, 1.36MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   1% 84.1M/9.97G [00:02<03:34, 46.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   1% 135M/9.97G [00:02<02:05, 78.6MB/s] \u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   3% 269M/9.97G [00:02<00:50, 192MB/s] \u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   2% 69.3M/3.85G [00:02<01:33, 40.7MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  21% 807M/3.85G [00:02<00:05, 581MB/s]  \u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   4% 403M/9.97G [00:02<00:36, 265MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   5% 470M/9.97G [00:03<00:47, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   6% 604M/9.97G [00:04<00:53, 174MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   7% 671M/9.97G [00:05<01:02, 149MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  26% 1.01G/3.85G [00:05<00:12, 222MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   8% 805M/9.97G [00:05<00:43, 212MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   9% 872M/9.97G [00:06<01:00, 151MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  10% 956M/9.97G [00:08<01:43, 86.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  30% 1.14G/3.85G [00:08<00:22, 122MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  10% 1.01G/9.97G [00:08<01:38, 91.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.07G/9.97G [00:09<01:54, 77.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  32% 1.22G/3.85G [00:12<00:37, 70.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.16G/9.97G [00:12<02:42, 54.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  33% 1.29G/3.85G [00:12<00:32, 79.9MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  37% 1.42G/3.85G [00:13<00:25, 95.2MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  39% 1.51G/3.85G [00:13<00:21, 108MB/s] \u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  43% 1.65G/3.85G [00:13<00:14, 150MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  46% 1.78G/3.85G [00:14<00:10, 193MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  50% 1.91G/3.85G [00:14<00:08, 216MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  53% 2.05G/3.85G [00:14<00:07, 254MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.22G/9.97G [00:15<03:41, 39.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.29G/9.97G [00:15<02:42, 53.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  57% 2.18G/3.85G [00:15<00:08, 189MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.36G/9.97G [00:17<02:58, 48.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.43G/9.97G [00:17<02:19, 61.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  58% 2.25G/3.85G [00:17<00:14, 109MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.63G/9.97G [00:17<01:07, 124MB/s] \u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.76G/9.97G [00:17<00:45, 180MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  20% 2.03G/9.97G [00:18<00:24, 323MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  60% 2.32G/3.85G [00:18<00:12, 119MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  62% 2.38G/3.85G [00:18<00:11, 126MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  64% 2.45G/3.85G [00:18<00:09, 154MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.16G/9.97G [00:19<00:34, 227MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  65% 2.52G/3.85G [00:19<00:08, 149MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  67% 2.58G/3.85G [00:19<00:08, 156MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.30G/9.97G [00:19<00:38, 202MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  69% 2.65G/3.85G [00:20<00:08, 135MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.36G/9.97G [00:20<00:38, 199MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  71% 2.72G/3.85G [00:20<00:08, 135MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  71% 2.75G/3.85G [00:21<00:08, 134MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.43G/9.97G [00:23<01:31, 82.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  73% 2.82G/3.85G [00:23<00:16, 61.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.57G/9.97G [00:24<01:29, 82.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  74% 2.85G/3.85G [00:25<00:23, 43.0MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  75% 2.88G/3.85G [00:25<00:18, 52.7MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  77% 2.95G/3.85G [00:26<00:14, 60.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.63G/9.97G [00:26<01:49, 67.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  78% 3.02G/3.85G [00:27<00:13, 59.7MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  80% 3.09G/3.85G [00:27<00:09, 78.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.70G/9.97G [00:27<01:45, 69.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.77G/9.97G [00:27<01:24, 85.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  81% 3.13G/3.85G [00:28<00:09, 72.7MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  83% 3.20G/3.85G [00:28<00:06, 96.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.83G/9.97G [00:28<01:25, 83.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  85% 3.27G/3.85G [00:28<00:04, 126MB/s] \u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  30% 2.97G/9.97G [00:29<01:00, 115MB/s] \u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  88% 3.38G/3.85G [00:29<00:02, 161MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  30% 3.04G/9.97G [00:29<00:53, 130MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.15G/9.97G [00:30<00:49, 137MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  90% 3.45G/3.85G [00:30<00:04, 98.9MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  91% 3.52G/3.85G [00:32<00:04, 69.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.24G/9.97G [00:33<01:46, 63.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  93% 3.58G/3.85G [00:33<00:04, 60.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.30G/9.97G [00:33<01:35, 70.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.44G/9.97G [00:34<00:58, 111MB/s] \u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  95% 3.65G/3.85G [00:34<00:02, 72.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.57G/9.97G [00:34<00:38, 165MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.64G/9.97G [00:34<00:33, 186MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  98% 3.79G/3.85G [00:34<00:00, 111MB/s] \u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.71G/9.97G [00:35<00:38, 164MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin: 100% 3.85G/3.85G [00:37<00:00, 102MB/s] \n",
            "\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.77G/9.97G [00:37<01:25, 72.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  40% 3.97G/9.97G [00:37<00:42, 141MB/s] \u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.04G/9.97G [00:39<00:59, 100MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.64G/9.97G [00:39<00:19, 276MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.78G/9.97G [00:40<00:17, 296MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.85G/9.97G [00:41<00:23, 216MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.91G/9.97G [00:41<00:26, 188MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.98G/9.97G [00:42<00:26, 185MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.05G/9.97G [00:42<00:27, 179MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.11G/9.97G [00:42<00:23, 210MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.20G/9.97G [00:43<00:26, 183MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.33G/9.97G [00:43<00:17, 262MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.40G/9.97G [00:43<00:19, 240MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.47G/9.97G [00:43<00:16, 266MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.53G/9.97G [00:44<00:14, 309MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.62G/9.97G [00:44<00:20, 210MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.67G/9.97G [00:45<00:22, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.74G/9.97G [00:45<00:20, 207MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.81G/9.97G [00:47<00:57, 72.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  60% 5.94G/9.97G [00:47<00:31, 127MB/s] \u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.14G/9.97G [00:47<00:16, 229MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.27G/9.97G [00:48<00:15, 235MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.36G/9.97G [00:48<00:15, 228MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.50G/9.97G [00:51<00:35, 98.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.61G/9.97G [00:51<00:25, 133MB/s] \u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.77G/9.97G [00:52<00:16, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.90G/9.97G [00:52<00:11, 260MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.10G/9.97G [00:53<00:12, 237MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.17G/9.97G [00:53<00:11, 250MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.24G/9.97G [00:53<00:10, 267MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.30G/9.97G [00:53<00:09, 284MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.37G/9.97G [00:54<00:15, 165MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.44G/9.97G [00:55<00:22, 111MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.58G/9.97G [00:56<00:13, 182MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.78G/9.97G [00:56<00:07, 305MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.89G/9.97G [00:56<00:06, 341MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  80% 8.02G/9.97G [00:57<00:07, 247MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.09G/9.97G [00:57<00:07, 236MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.16G/9.97G [00:57<00:07, 246MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.23G/9.97G [00:58<00:08, 211MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.29G/9.97G [00:58<00:09, 171MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.36G/9.97G [00:59<00:08, 191MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.43G/9.97G [00:59<00:07, 219MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.49G/9.97G [00:59<00:08, 167MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.56G/9.97G [01:00<00:08, 169MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.63G/9.97G [01:00<00:08, 166MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.76G/9.97G [01:01<00:07, 161MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.83G/9.97G [01:02<00:09, 115MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.90G/9.97G [01:04<00:12, 84.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.96G/9.97G [01:05<00:15, 63.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.03G/9.97G [01:06<00:12, 75.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.16G/9.97G [01:06<00:06, 122MB/s] \u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.23G/9.97G [01:06<00:05, 146MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.30G/9.97G [01:07<00:05, 114MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.36G/9.97G [01:08<00:04, 132MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.43G/9.97G [01:08<00:03, 161MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.50G/9.97G [01:08<00:02, 196MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.57G/9.97G [01:08<00:01, 236MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.63G/9.97G [01:08<00:01, 282MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.77G/9.97G [01:08<00:00, 388MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin: 100% 9.97G/9.97G [01:09<00:00, 144MB/s]\n",
            "Fetching 2 files: 100% 2/2 [01:09<00:00, 34.73s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [00:15<00:00,  7.71s/it]\n",
            "generation_config.json: 100% 121/121 [00:00<00:00, 524kB/s]\n",
            "trainable params: 15,728,640 || all params: 6,926,094,336 || trainable%: 0.2271\n",
            "ğŸš€ Training DeepSeek (15-20h)...\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 100001, 'bos_token_id': 100000, 'pad_token_id': 100001}.\n",
            "  0% 0/255 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "{'loss': 2.6013, 'grad_norm': 1.6844861507415771, 'learning_rate': 0.00014615384615384615, 'epoch': 0.39}\n",
            "{'loss': 0.5615, 'grad_norm': 0.9402686953544617, 'learning_rate': 0.000188646288209607, 'epoch': 0.78}\n",
            " 20% 51/255 [04:16<16:56,  4.99s/it]\n",
            "  0% 0/102 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/102 [00:00<00:11,  8.65it/s]\u001b[A\n",
            "  3% 3/102 [00:00<00:16,  6.08it/s]\u001b[A\n",
            "  4% 4/102 [00:00<00:18,  5.33it/s]\u001b[A\n",
            "  5% 5/102 [00:00<00:19,  4.96it/s]\u001b[A\n",
            "  6% 6/102 [00:01<00:20,  4.76it/s]\u001b[A\n",
            "  7% 7/102 [00:01<00:20,  4.65it/s]\u001b[A\n",
            "  8% 8/102 [00:01<00:20,  4.58it/s]\u001b[A\n",
            "  9% 9/102 [00:01<00:20,  4.52it/s]\u001b[A\n",
            " 10% 10/102 [00:02<00:20,  4.48it/s]\u001b[A\n",
            " 11% 11/102 [00:02<00:20,  4.47it/s]\u001b[A\n",
            " 12% 12/102 [00:02<00:20,  4.45it/s]\u001b[A\n",
            " 13% 13/102 [00:02<00:19,  4.47it/s]\u001b[A\n",
            " 14% 14/102 [00:02<00:19,  4.48it/s]\u001b[A\n",
            " 15% 15/102 [00:03<00:19,  4.48it/s]\u001b[A\n",
            " 16% 16/102 [00:03<00:19,  4.44it/s]\u001b[A\n",
            " 17% 17/102 [00:03<00:19,  4.41it/s]\u001b[A\n",
            " 18% 18/102 [00:03<00:19,  4.41it/s]\u001b[A\n",
            " 19% 19/102 [00:04<00:18,  4.42it/s]\u001b[A\n",
            " 20% 20/102 [00:04<00:18,  4.43it/s]\u001b[A\n",
            " 21% 21/102 [00:04<00:18,  4.42it/s]\u001b[A\n",
            " 22% 22/102 [00:04<00:18,  4.43it/s]\u001b[A\n",
            " 23% 23/102 [00:04<00:17,  4.42it/s]\u001b[A\n",
            " 24% 24/102 [00:05<00:17,  4.43it/s]\u001b[A\n",
            " 25% 25/102 [00:05<00:17,  4.43it/s]\u001b[A\n",
            " 25% 26/102 [00:05<00:17,  4.42it/s]\u001b[A\n",
            " 26% 27/102 [00:05<00:16,  4.42it/s]\u001b[A\n",
            " 27% 28/102 [00:06<00:16,  4.42it/s]\u001b[A\n",
            " 28% 29/102 [00:06<00:16,  4.43it/s]\u001b[A\n",
            " 29% 30/102 [00:06<00:16,  4.28it/s]\u001b[A\n",
            " 30% 31/102 [00:06<00:16,  4.33it/s]\u001b[A\n",
            " 31% 32/102 [00:07<00:16,  4.36it/s]\u001b[A\n",
            " 32% 33/102 [00:07<00:15,  4.39it/s]\u001b[A\n",
            " 33% 34/102 [00:07<00:15,  4.40it/s]\u001b[A\n",
            " 34% 35/102 [00:07<00:15,  4.41it/s]\u001b[A\n",
            " 35% 36/102 [00:07<00:14,  4.43it/s]\u001b[A\n",
            " 36% 37/102 [00:08<00:14,  4.43it/s]\u001b[A\n",
            " 37% 38/102 [00:08<00:14,  4.43it/s]\u001b[A\n",
            " 38% 39/102 [00:08<00:14,  4.32it/s]\u001b[A\n",
            " 39% 40/102 [00:08<00:14,  4.29it/s]\u001b[A\n",
            " 40% 41/102 [00:09<00:14,  4.35it/s]\u001b[A\n",
            " 41% 42/102 [00:09<00:13,  4.38it/s]\u001b[A\n",
            " 42% 43/102 [00:09<00:13,  4.35it/s]\u001b[A\n",
            " 43% 44/102 [00:09<00:13,  4.32it/s]\u001b[A\n",
            " 44% 45/102 [00:10<00:13,  4.37it/s]\u001b[A\n",
            " 45% 46/102 [00:10<00:12,  4.39it/s]\u001b[A\n",
            " 46% 47/102 [00:10<00:12,  4.40it/s]\u001b[A\n",
            " 47% 48/102 [00:10<00:12,  4.44it/s]\u001b[A\n",
            " 48% 49/102 [00:10<00:11,  4.44it/s]\u001b[A\n",
            " 49% 50/102 [00:11<00:11,  4.42it/s]\u001b[A\n",
            " 50% 51/102 [00:11<00:11,  4.40it/s]\u001b[A\n",
            " 51% 52/102 [00:11<00:11,  4.34it/s]\u001b[A\n",
            " 52% 53/102 [00:11<00:11,  4.30it/s]\u001b[A\n",
            " 53% 54/102 [00:12<00:11,  4.32it/s]\u001b[A\n",
            " 54% 55/102 [00:12<00:10,  4.35it/s]\u001b[A\n",
            " 55% 56/102 [00:12<00:10,  4.32it/s]\u001b[A\n",
            " 56% 57/102 [00:12<00:10,  4.33it/s]\u001b[A\n",
            " 57% 58/102 [00:12<00:10,  4.36it/s]\u001b[A\n",
            " 58% 59/102 [00:13<00:09,  4.38it/s]\u001b[A\n",
            " 59% 60/102 [00:13<00:09,  4.39it/s]\u001b[A\n",
            " 60% 61/102 [00:13<00:09,  4.39it/s]\u001b[A\n",
            " 61% 62/102 [00:13<00:09,  4.40it/s]\u001b[A\n",
            " 62% 63/102 [00:14<00:08,  4.41it/s]\u001b[A\n",
            " 63% 64/102 [00:14<00:08,  4.41it/s]\u001b[A\n",
            " 64% 65/102 [00:14<00:08,  4.42it/s]\u001b[A\n",
            " 65% 66/102 [00:14<00:08,  4.41it/s]\u001b[A\n",
            " 66% 67/102 [00:15<00:07,  4.41it/s]\u001b[A\n",
            " 67% 68/102 [00:15<00:07,  4.44it/s]\u001b[A\n",
            " 68% 69/102 [00:15<00:07,  4.44it/s]\u001b[A\n",
            " 69% 70/102 [00:15<00:07,  4.43it/s]\u001b[A\n",
            " 70% 71/102 [00:15<00:06,  4.43it/s]\u001b[A\n",
            " 71% 72/102 [00:16<00:06,  4.45it/s]\u001b[A\n",
            " 72% 73/102 [00:16<00:06,  4.45it/s]\u001b[A\n",
            " 73% 74/102 [00:16<00:06,  4.42it/s]\u001b[A\n",
            " 74% 75/102 [00:16<00:06,  4.43it/s]\u001b[A\n",
            " 75% 76/102 [00:17<00:05,  4.43it/s]\u001b[A\n",
            " 75% 77/102 [00:17<00:05,  4.43it/s]\u001b[A\n",
            " 76% 78/102 [00:17<00:05,  4.43it/s]\u001b[A\n",
            " 77% 79/102 [00:17<00:05,  4.42it/s]\u001b[A\n",
            " 78% 80/102 [00:17<00:04,  4.43it/s]\u001b[A\n",
            " 79% 81/102 [00:18<00:04,  4.44it/s]\u001b[A\n",
            " 80% 82/102 [00:18<00:04,  4.42it/s]\u001b[A\n",
            " 81% 83/102 [00:18<00:04,  4.43it/s]\u001b[A\n",
            " 82% 84/102 [00:18<00:04,  4.44it/s]\u001b[A\n",
            " 83% 85/102 [00:19<00:03,  4.45it/s]\u001b[A\n",
            " 84% 86/102 [00:19<00:03,  4.44it/s]\u001b[A\n",
            " 85% 87/102 [00:19<00:03,  4.45it/s]\u001b[A\n",
            " 86% 88/102 [00:19<00:03,  4.46it/s]\u001b[A\n",
            " 87% 89/102 [00:19<00:02,  4.46it/s]\u001b[A\n",
            " 88% 90/102 [00:20<00:02,  4.43it/s]\u001b[A\n",
            " 89% 91/102 [00:20<00:02,  4.42it/s]\u001b[A\n",
            " 90% 92/102 [00:20<00:02,  4.41it/s]\u001b[A\n",
            " 91% 93/102 [00:20<00:02,  4.41it/s]\u001b[A\n",
            " 92% 94/102 [00:21<00:01,  4.43it/s]\u001b[A\n",
            " 93% 95/102 [00:21<00:01,  4.40it/s]\u001b[A\n",
            " 94% 96/102 [00:21<00:01,  4.42it/s]\u001b[A\n",
            " 95% 97/102 [00:21<00:01,  4.43it/s]\u001b[A\n",
            " 96% 98/102 [00:22<00:00,  4.45it/s]\u001b[A\n",
            " 97% 99/102 [00:22<00:00,  4.45it/s]\u001b[A\n",
            " 98% 100/102 [00:22<00:00,  4.44it/s]\u001b[A\n",
            " 99% 101/102 [00:22<00:00,  4.42it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.19221100211143494, 'eval_runtime': 23.1627, 'eval_samples_per_second': 8.807, 'eval_steps_per_second': 4.404, 'epoch': 1.0}\n",
            " 20% 51/255 [04:39<16:56,  4.99s/it]\n",
            "100% 102/102 [00:22<00:00,  4.42it/s]\u001b[A\n",
            "{'loss': 0.1971, 'grad_norm': 0.6221886873245239, 'learning_rate': 0.00017117903930131005, 'epoch': 1.18}\n",
            "{'loss': 0.1755, 'grad_norm': 0.42840033769607544, 'learning_rate': 0.00015371179039301312, 'epoch': 1.57}\n",
            "{'loss': 0.1721, 'grad_norm': 0.5882996320724487, 'learning_rate': 0.00013624454148471616, 'epoch': 1.96}\n",
            " 40% 102/255 [08:54<12:46,  5.01s/it]\n",
            "  0% 0/102 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/102 [00:00<00:11,  8.81it/s]\u001b[A\n",
            "  3% 3/102 [00:00<00:16,  6.02it/s]\u001b[A\n",
            "  4% 4/102 [00:00<00:18,  5.29it/s]\u001b[A\n",
            "  5% 5/102 [00:00<00:19,  4.96it/s]\u001b[A\n",
            "  6% 6/102 [00:01<00:20,  4.78it/s]\u001b[A\n",
            "  7% 7/102 [00:01<00:20,  4.67it/s]\u001b[A\n",
            "  8% 8/102 [00:01<00:20,  4.60it/s]\u001b[A\n",
            "  9% 9/102 [00:01<00:20,  4.52it/s]\u001b[A\n",
            " 10% 10/102 [00:02<00:20,  4.49it/s]\u001b[A\n",
            " 11% 11/102 [00:02<00:20,  4.48it/s]\u001b[A\n",
            " 12% 12/102 [00:02<00:20,  4.41it/s]\u001b[A\n",
            " 13% 13/102 [00:02<00:20,  4.39it/s]\u001b[A\n",
            " 14% 14/102 [00:02<00:20,  4.38it/s]\u001b[A\n",
            " 15% 15/102 [00:03<00:19,  4.40it/s]\u001b[A\n",
            " 16% 16/102 [00:03<00:19,  4.39it/s]\u001b[A\n",
            " 17% 17/102 [00:03<00:19,  4.36it/s]\u001b[A\n",
            " 18% 18/102 [00:03<00:19,  4.37it/s]\u001b[A\n",
            " 19% 19/102 [00:04<00:18,  4.37it/s]\u001b[A\n",
            " 20% 20/102 [00:04<00:18,  4.36it/s]\u001b[A\n",
            " 21% 21/102 [00:04<00:18,  4.32it/s]\u001b[A\n",
            " 22% 22/102 [00:04<00:18,  4.34it/s]\u001b[A\n",
            " 23% 23/102 [00:05<00:18,  4.36it/s]\u001b[A\n",
            " 24% 24/102 [00:05<00:17,  4.36it/s]\u001b[A\n",
            " 25% 25/102 [00:05<00:17,  4.36it/s]\u001b[A\n",
            " 25% 26/102 [00:05<00:17,  4.34it/s]\u001b[A\n",
            " 26% 27/102 [00:05<00:17,  4.33it/s]\u001b[A\n",
            " 27% 28/102 [00:06<00:17,  4.30it/s]\u001b[A\n",
            " 28% 29/102 [00:06<00:17,  4.25it/s]\u001b[A\n",
            " 29% 30/102 [00:06<00:17,  4.22it/s]\u001b[A\n",
            " 30% 31/102 [00:06<00:16,  4.26it/s]\u001b[A\n",
            " 31% 32/102 [00:07<00:16,  4.29it/s]\u001b[A\n",
            " 32% 33/102 [00:07<00:16,  4.27it/s]\u001b[A\n",
            " 33% 34/102 [00:07<00:16,  4.19it/s]\u001b[A\n",
            " 34% 35/102 [00:07<00:15,  4.23it/s]\u001b[A\n",
            " 35% 36/102 [00:08<00:15,  4.24it/s]\u001b[A\n",
            " 36% 37/102 [00:08<00:15,  4.24it/s]\u001b[A\n",
            " 37% 38/102 [00:08<00:15,  4.25it/s]\u001b[A\n",
            " 38% 39/102 [00:08<00:14,  4.30it/s]\u001b[A\n",
            " 39% 40/102 [00:09<00:14,  4.32it/s]\u001b[A\n",
            " 40% 41/102 [00:09<00:14,  4.32it/s]\u001b[A\n",
            " 41% 42/102 [00:09<00:13,  4.35it/s]\u001b[A\n",
            " 42% 43/102 [00:09<00:13,  4.35it/s]\u001b[A\n",
            " 43% 44/102 [00:09<00:13,  4.36it/s]\u001b[A\n",
            " 44% 45/102 [00:10<00:13,  4.38it/s]\u001b[A\n",
            " 45% 46/102 [00:10<00:12,  4.38it/s]\u001b[A\n",
            " 46% 47/102 [00:10<00:12,  4.38it/s]\u001b[A\n",
            " 47% 48/102 [00:10<00:12,  4.40it/s]\u001b[A\n",
            " 48% 49/102 [00:11<00:12,  4.38it/s]\u001b[A\n",
            " 49% 50/102 [00:11<00:11,  4.39it/s]\u001b[A\n",
            " 50% 51/102 [00:11<00:11,  4.33it/s]\u001b[A\n",
            " 51% 52/102 [00:11<00:11,  4.35it/s]\u001b[A\n",
            " 52% 53/102 [00:11<00:11,  4.37it/s]\u001b[A\n",
            " 53% 54/102 [00:12<00:10,  4.39it/s]\u001b[A\n",
            " 54% 55/102 [00:12<00:10,  4.39it/s]\u001b[A\n",
            " 55% 56/102 [00:12<00:10,  4.40it/s]\u001b[A\n",
            " 56% 57/102 [00:12<00:10,  4.42it/s]\u001b[A\n",
            " 57% 58/102 [00:13<00:09,  4.42it/s]\u001b[A\n",
            " 58% 59/102 [00:13<00:09,  4.42it/s]\u001b[A\n",
            " 59% 60/102 [00:13<00:09,  4.42it/s]\u001b[A\n",
            " 60% 61/102 [00:13<00:09,  4.42it/s]\u001b[A\n",
            " 61% 62/102 [00:14<00:09,  4.42it/s]\u001b[A\n",
            " 62% 63/102 [00:14<00:08,  4.43it/s]\u001b[A\n",
            " 63% 64/102 [00:14<00:08,  4.40it/s]\u001b[A\n",
            " 64% 65/102 [00:14<00:08,  4.34it/s]\u001b[A\n",
            " 65% 66/102 [00:14<00:08,  4.36it/s]\u001b[A\n",
            " 66% 67/102 [00:15<00:07,  4.38it/s]\u001b[A\n",
            " 67% 68/102 [00:15<00:07,  4.40it/s]\u001b[A\n",
            " 68% 69/102 [00:15<00:07,  4.39it/s]\u001b[A\n",
            " 69% 70/102 [00:15<00:07,  4.40it/s]\u001b[A\n",
            " 70% 71/102 [00:16<00:07,  4.41it/s]\u001b[A\n",
            " 71% 72/102 [00:16<00:06,  4.41it/s]\u001b[A\n",
            " 72% 73/102 [00:16<00:06,  4.40it/s]\u001b[A\n",
            " 73% 74/102 [00:16<00:06,  4.40it/s]\u001b[A\n",
            " 74% 75/102 [00:16<00:06,  4.35it/s]\u001b[A\n",
            " 75% 76/102 [00:17<00:05,  4.37it/s]\u001b[A\n",
            " 75% 77/102 [00:17<00:05,  4.36it/s]\u001b[A\n",
            " 76% 78/102 [00:17<00:05,  4.31it/s]\u001b[A\n",
            " 77% 79/102 [00:17<00:05,  4.32it/s]\u001b[A\n",
            " 78% 80/102 [00:18<00:05,  4.36it/s]\u001b[A\n",
            " 79% 81/102 [00:18<00:04,  4.32it/s]\u001b[A\n",
            " 80% 82/102 [00:18<00:04,  4.31it/s]\u001b[A\n",
            " 81% 83/102 [00:18<00:04,  4.30it/s]\u001b[A\n",
            " 82% 84/102 [00:19<00:04,  4.29it/s]\u001b[A\n",
            " 83% 85/102 [00:19<00:03,  4.30it/s]\u001b[A\n",
            " 84% 86/102 [00:19<00:03,  4.21it/s]\u001b[A\n",
            " 85% 87/102 [00:19<00:03,  4.23it/s]\u001b[A\n",
            " 86% 88/102 [00:20<00:03,  4.26it/s]\u001b[A\n",
            " 87% 89/102 [00:20<00:03,  4.21it/s]\u001b[A\n",
            " 88% 90/102 [00:20<00:02,  4.11it/s]\u001b[A\n",
            " 89% 91/102 [00:20<00:02,  4.16it/s]\u001b[A\n",
            " 90% 92/102 [00:20<00:02,  4.23it/s]\u001b[A\n",
            " 91% 93/102 [00:21<00:02,  4.28it/s]\u001b[A\n",
            " 92% 94/102 [00:21<00:01,  4.32it/s]\u001b[A\n",
            " 93% 95/102 [00:21<00:01,  4.35it/s]\u001b[A\n",
            " 94% 96/102 [00:21<00:01,  4.39it/s]\u001b[A\n",
            " 95% 97/102 [00:22<00:01,  4.41it/s]\u001b[A\n",
            " 96% 98/102 [00:22<00:00,  4.41it/s]\u001b[A\n",
            " 97% 99/102 [00:22<00:00,  4.34it/s]\u001b[A\n",
            " 98% 100/102 [00:22<00:00,  4.34it/s]\u001b[A\n",
            " 99% 101/102 [00:23<00:00,  4.36it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.1677466779947281, 'eval_runtime': 23.4964, 'eval_samples_per_second': 8.682, 'eval_steps_per_second': 4.341, 'epoch': 2.0}\n",
            " 40% 102/255 [09:17<12:46,  5.01s/it]\n",
            "100% 102/102 [00:23<00:00,  4.37it/s]\u001b[A\n",
            "{'loss': 0.163, 'grad_norm': 0.2598904073238373, 'learning_rate': 0.00011877729257641923, 'epoch': 2.35}\n",
            "{'loss': 0.1605, 'grad_norm': 0.26009348034858704, 'learning_rate': 0.00010131004366812226, 'epoch': 2.75}\n",
            " 60% 153/255 [13:33<08:30,  5.01s/it]\n",
            "  0% 0/102 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/102 [00:00<00:11,  8.71it/s]\u001b[A\n",
            "  3% 3/102 [00:00<00:16,  6.16it/s]\u001b[A\n",
            "  4% 4/102 [00:00<00:18,  5.35it/s]\u001b[A\n",
            "  5% 5/102 [00:00<00:19,  5.01it/s]\u001b[A\n",
            "  6% 6/102 [00:01<00:20,  4.75it/s]\u001b[A\n",
            "  7% 7/102 [00:01<00:20,  4.59it/s]\u001b[A\n",
            "  8% 8/102 [00:01<00:20,  4.51it/s]\u001b[A\n",
            "  9% 9/102 [00:01<00:20,  4.47it/s]\u001b[A\n",
            " 10% 10/102 [00:02<00:20,  4.45it/s]\u001b[A\n",
            " 11% 11/102 [00:02<00:20,  4.33it/s]\u001b[A\n",
            " 12% 12/102 [00:02<00:20,  4.36it/s]\u001b[A\n",
            " 13% 13/102 [00:02<00:20,  4.36it/s]\u001b[A\n",
            " 14% 14/102 [00:02<00:20,  4.37it/s]\u001b[A\n",
            " 15% 15/102 [00:03<00:19,  4.39it/s]\u001b[A\n",
            " 16% 16/102 [00:03<00:19,  4.38it/s]\u001b[A\n",
            " 17% 17/102 [00:03<00:19,  4.39it/s]\u001b[A\n",
            " 18% 18/102 [00:03<00:19,  4.40it/s]\u001b[A\n",
            " 19% 19/102 [00:04<00:18,  4.41it/s]\u001b[A\n",
            " 20% 20/102 [00:04<00:18,  4.34it/s]\u001b[A\n",
            " 21% 21/102 [00:04<00:18,  4.36it/s]\u001b[A\n",
            " 22% 22/102 [00:04<00:18,  4.36it/s]\u001b[A\n",
            " 23% 23/102 [00:05<00:18,  4.37it/s]\u001b[A\n",
            " 24% 24/102 [00:05<00:17,  4.38it/s]\u001b[A\n",
            " 25% 25/102 [00:05<00:17,  4.39it/s]\u001b[A\n",
            " 25% 26/102 [00:05<00:17,  4.39it/s]\u001b[A\n",
            " 26% 27/102 [00:05<00:17,  4.39it/s]\u001b[A\n",
            " 27% 28/102 [00:06<00:17,  4.32it/s]\u001b[A\n",
            " 28% 29/102 [00:06<00:17,  4.22it/s]\u001b[A\n",
            " 29% 30/102 [00:06<00:16,  4.28it/s]\u001b[A\n",
            " 30% 31/102 [00:06<00:16,  4.30it/s]\u001b[A\n",
            " 31% 32/102 [00:07<00:16,  4.34it/s]\u001b[A\n",
            " 32% 33/102 [00:07<00:15,  4.35it/s]\u001b[A\n",
            " 33% 34/102 [00:07<00:15,  4.37it/s]\u001b[A\n",
            " 34% 35/102 [00:07<00:15,  4.37it/s]\u001b[A\n",
            " 35% 36/102 [00:08<00:15,  4.31it/s]\u001b[A\n",
            " 36% 37/102 [00:08<00:15,  4.28it/s]\u001b[A\n",
            " 37% 38/102 [00:08<00:14,  4.31it/s]\u001b[A\n",
            " 38% 39/102 [00:08<00:14,  4.35it/s]\u001b[A\n",
            " 39% 40/102 [00:08<00:14,  4.37it/s]\u001b[A\n",
            " 40% 41/102 [00:09<00:13,  4.37it/s]\u001b[A\n",
            " 41% 42/102 [00:09<00:13,  4.39it/s]\u001b[A\n",
            " 42% 43/102 [00:09<00:13,  4.37it/s]\u001b[A\n",
            " 43% 44/102 [00:09<00:13,  4.38it/s]\u001b[A\n",
            " 44% 45/102 [00:10<00:13,  4.38it/s]\u001b[A\n",
            " 45% 46/102 [00:10<00:12,  4.36it/s]\u001b[A\n",
            " 46% 47/102 [00:10<00:12,  4.34it/s]\u001b[A\n",
            " 47% 48/102 [00:10<00:12,  4.26it/s]\u001b[A\n",
            " 48% 49/102 [00:11<00:12,  4.27it/s]\u001b[A\n",
            " 49% 50/102 [00:11<00:12,  4.25it/s]\u001b[A\n",
            " 50% 51/102 [00:11<00:11,  4.26it/s]\u001b[A\n",
            " 51% 52/102 [00:11<00:11,  4.19it/s]\u001b[A\n",
            " 52% 53/102 [00:12<00:11,  4.23it/s]\u001b[A\n",
            " 53% 54/102 [00:12<00:11,  4.24it/s]\u001b[A\n",
            " 54% 55/102 [00:12<00:11,  4.25it/s]\u001b[A\n",
            " 55% 56/102 [00:12<00:10,  4.25it/s]\u001b[A\n",
            " 56% 57/102 [00:12<00:10,  4.31it/s]\u001b[A\n",
            " 57% 58/102 [00:13<00:10,  4.33it/s]\u001b[A\n",
            " 58% 59/102 [00:13<00:09,  4.35it/s]\u001b[A\n",
            " 59% 60/102 [00:13<00:09,  4.35it/s]\u001b[A\n",
            " 60% 61/102 [00:13<00:09,  4.34it/s]\u001b[A\n",
            " 61% 62/102 [00:14<00:09,  4.26it/s]\u001b[A\n",
            " 62% 63/102 [00:14<00:09,  4.29it/s]\u001b[A\n",
            " 63% 64/102 [00:14<00:08,  4.31it/s]\u001b[A\n",
            " 64% 65/102 [00:14<00:08,  4.33it/s]\u001b[A\n",
            " 65% 66/102 [00:15<00:08,  4.28it/s]\u001b[A\n",
            " 66% 67/102 [00:15<00:08,  4.29it/s]\u001b[A\n",
            " 67% 68/102 [00:15<00:07,  4.27it/s]\u001b[A\n",
            " 68% 69/102 [00:15<00:07,  4.30it/s]\u001b[A\n",
            " 69% 70/102 [00:15<00:07,  4.32it/s]\u001b[A\n",
            " 70% 71/102 [00:16<00:07,  4.29it/s]\u001b[A\n",
            " 71% 72/102 [00:16<00:07,  4.25it/s]\u001b[A\n",
            " 72% 73/102 [00:16<00:06,  4.31it/s]\u001b[A\n",
            " 73% 74/102 [00:16<00:06,  4.34it/s]\u001b[A\n",
            " 74% 75/102 [00:17<00:06,  4.36it/s]\u001b[A\n",
            " 75% 76/102 [00:17<00:06,  4.31it/s]\u001b[A\n",
            " 75% 77/102 [00:17<00:05,  4.34it/s]\u001b[A\n",
            " 76% 78/102 [00:17<00:05,  4.36it/s]\u001b[A\n",
            " 77% 79/102 [00:18<00:05,  4.34it/s]\u001b[A\n",
            " 78% 80/102 [00:18<00:05,  4.36it/s]\u001b[A\n",
            " 79% 81/102 [00:18<00:04,  4.39it/s]\u001b[A\n",
            " 80% 82/102 [00:18<00:04,  4.41it/s]\u001b[A\n",
            " 81% 83/102 [00:18<00:04,  4.33it/s]\u001b[A\n",
            " 82% 84/102 [00:19<00:04,  4.36it/s]\u001b[A\n",
            " 83% 85/102 [00:19<00:03,  4.37it/s]\u001b[A\n",
            " 84% 86/102 [00:19<00:03,  4.38it/s]\u001b[A\n",
            " 85% 87/102 [00:19<00:03,  4.38it/s]\u001b[A\n",
            " 86% 88/102 [00:20<00:03,  4.38it/s]\u001b[A\n",
            " 87% 89/102 [00:20<00:02,  4.38it/s]\u001b[A\n",
            " 88% 90/102 [00:20<00:02,  4.39it/s]\u001b[A\n",
            " 89% 91/102 [00:20<00:02,  4.39it/s]\u001b[A\n",
            " 90% 92/102 [00:20<00:02,  4.36it/s]\u001b[A\n",
            " 91% 93/102 [00:21<00:02,  4.33it/s]\u001b[A\n",
            " 92% 94/102 [00:21<00:01,  4.36it/s]\u001b[A\n",
            " 93% 95/102 [00:21<00:01,  4.38it/s]\u001b[A\n",
            " 94% 96/102 [00:21<00:01,  4.38it/s]\u001b[A\n",
            " 95% 97/102 [00:22<00:01,  4.37it/s]\u001b[A\n",
            " 96% 98/102 [00:22<00:00,  4.32it/s]\u001b[A\n",
            " 97% 99/102 [00:22<00:00,  4.36it/s]\u001b[A\n",
            " 98% 100/102 [00:22<00:00,  4.33it/s]\u001b[A\n",
            " 99% 101/102 [00:23<00:00,  4.33it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.16348041594028473, 'eval_runtime': 23.5571, 'eval_samples_per_second': 8.66, 'eval_steps_per_second': 4.33, 'epoch': 3.0}\n",
            " 60% 153/255 [13:57<08:30,  5.01s/it]\n",
            "100% 102/102 [00:23<00:00,  4.28it/s]\u001b[A\n",
            "{'loss': 0.1605, 'grad_norm': 0.3699371814727783, 'learning_rate': 8.384279475982532e-05, 'epoch': 3.14}\n",
            "{'loss': 0.1587, 'grad_norm': 0.24957279860973358, 'learning_rate': 6.637554585152839e-05, 'epoch': 3.53}\n",
            "{'loss': 0.1585, 'grad_norm': 0.24223721027374268, 'learning_rate': 4.890829694323144e-05, 'epoch': 3.92}\n",
            " 80% 204/255 [18:13<04:14,  4.99s/it]\n",
            "  0% 0/102 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/102 [00:00<00:11,  8.84it/s]\u001b[A\n",
            "  3% 3/102 [00:00<00:16,  6.09it/s]\u001b[A\n",
            "  4% 4/102 [00:00<00:18,  5.27it/s]\u001b[A\n",
            "  5% 5/102 [00:00<00:20,  4.82it/s]\u001b[A\n",
            "  6% 6/102 [00:01<00:20,  4.68it/s]\u001b[A\n",
            "  7% 7/102 [00:01<00:20,  4.60it/s]\u001b[A\n",
            "  8% 8/102 [00:01<00:21,  4.47it/s]\u001b[A\n",
            "  9% 9/102 [00:01<00:20,  4.45it/s]\u001b[A\n",
            " 10% 10/102 [00:02<00:20,  4.42it/s]\u001b[A\n",
            " 11% 11/102 [00:02<00:20,  4.43it/s]\u001b[A\n",
            " 12% 12/102 [00:02<00:20,  4.44it/s]\u001b[A\n",
            " 13% 13/102 [00:02<00:20,  4.43it/s]\u001b[A\n",
            " 14% 14/102 [00:02<00:19,  4.43it/s]\u001b[A\n",
            " 15% 15/102 [00:03<00:19,  4.43it/s]\u001b[A\n",
            " 16% 16/102 [00:03<00:19,  4.43it/s]\u001b[A\n",
            " 17% 17/102 [00:03<00:19,  4.40it/s]\u001b[A\n",
            " 18% 18/102 [00:03<00:19,  4.40it/s]\u001b[A\n",
            " 19% 19/102 [00:04<00:19,  4.34it/s]\u001b[A\n",
            " 20% 20/102 [00:04<00:18,  4.35it/s]\u001b[A\n",
            " 21% 21/102 [00:04<00:18,  4.35it/s]\u001b[A\n",
            " 22% 22/102 [00:04<00:18,  4.35it/s]\u001b[A\n",
            " 23% 23/102 [00:05<00:18,  4.37it/s]\u001b[A\n",
            " 24% 24/102 [00:05<00:17,  4.38it/s]\u001b[A\n",
            " 25% 25/102 [00:05<00:17,  4.36it/s]\u001b[A\n",
            " 25% 26/102 [00:05<00:17,  4.38it/s]\u001b[A\n",
            " 26% 27/102 [00:05<00:17,  4.34it/s]\u001b[A\n",
            " 27% 28/102 [00:06<00:17,  4.33it/s]\u001b[A\n",
            " 28% 29/102 [00:06<00:16,  4.34it/s]\u001b[A\n",
            " 29% 30/102 [00:06<00:16,  4.31it/s]\u001b[A\n",
            " 30% 31/102 [00:06<00:16,  4.32it/s]\u001b[A\n",
            " 31% 32/102 [00:07<00:16,  4.35it/s]\u001b[A\n",
            " 32% 33/102 [00:07<00:15,  4.37it/s]\u001b[A\n",
            " 33% 34/102 [00:07<00:15,  4.39it/s]\u001b[A\n",
            " 34% 35/102 [00:07<00:15,  4.39it/s]\u001b[A\n",
            " 35% 36/102 [00:08<00:15,  4.39it/s]\u001b[A\n",
            " 36% 37/102 [00:08<00:14,  4.40it/s]\u001b[A\n",
            " 37% 38/102 [00:08<00:14,  4.36it/s]\u001b[A\n",
            " 38% 39/102 [00:08<00:14,  4.27it/s]\u001b[A\n",
            " 39% 40/102 [00:08<00:14,  4.31it/s]\u001b[A\n",
            " 40% 41/102 [00:09<00:14,  4.34it/s]\u001b[A\n",
            " 41% 42/102 [00:09<00:13,  4.36it/s]\u001b[A\n",
            " 42% 43/102 [00:09<00:13,  4.37it/s]\u001b[A\n",
            " 43% 44/102 [00:09<00:13,  4.39it/s]\u001b[A\n",
            " 44% 45/102 [00:10<00:12,  4.42it/s]\u001b[A\n",
            " 45% 46/102 [00:10<00:12,  4.44it/s]\u001b[A\n",
            " 46% 47/102 [00:10<00:12,  4.43it/s]\u001b[A\n",
            " 47% 48/102 [00:10<00:12,  4.42it/s]\u001b[A\n",
            " 48% 49/102 [00:10<00:11,  4.44it/s]\u001b[A\n",
            " 49% 50/102 [00:11<00:11,  4.44it/s]\u001b[A\n",
            " 50% 51/102 [00:11<00:11,  4.46it/s]\u001b[A\n",
            " 51% 52/102 [00:11<00:11,  4.47it/s]\u001b[A\n",
            " 52% 53/102 [00:11<00:11,  4.38it/s]\u001b[A\n",
            " 53% 54/102 [00:12<00:10,  4.40it/s]\u001b[A\n",
            " 54% 55/102 [00:12<00:10,  4.39it/s]\u001b[A\n",
            " 55% 56/102 [00:12<00:10,  4.39it/s]\u001b[A\n",
            " 56% 57/102 [00:12<00:10,  4.37it/s]\u001b[A\n",
            " 57% 58/102 [00:13<00:10,  4.38it/s]\u001b[A\n",
            " 58% 59/102 [00:13<00:09,  4.39it/s]\u001b[A\n",
            " 59% 60/102 [00:13<00:09,  4.40it/s]\u001b[A\n",
            " 60% 61/102 [00:13<00:09,  4.38it/s]\u001b[A\n",
            " 61% 62/102 [00:13<00:09,  4.26it/s]\u001b[A\n",
            " 62% 63/102 [00:14<00:09,  4.31it/s]\u001b[A\n",
            " 63% 64/102 [00:14<00:08,  4.31it/s]\u001b[A\n",
            " 64% 65/102 [00:14<00:08,  4.35it/s]\u001b[A\n",
            " 65% 66/102 [00:14<00:08,  4.36it/s]\u001b[A\n",
            " 66% 67/102 [00:15<00:07,  4.40it/s]\u001b[A\n",
            " 67% 68/102 [00:15<00:07,  4.43it/s]\u001b[A\n",
            " 68% 69/102 [00:15<00:07,  4.45it/s]\u001b[A\n",
            " 69% 70/102 [00:15<00:07,  4.48it/s]\u001b[A\n",
            " 70% 71/102 [00:15<00:06,  4.49it/s]\u001b[A\n",
            " 71% 72/102 [00:16<00:06,  4.50it/s]\u001b[A\n",
            " 72% 73/102 [00:16<00:06,  4.49it/s]\u001b[A\n",
            " 73% 74/102 [00:16<00:06,  4.47it/s]\u001b[A\n",
            " 74% 75/102 [00:16<00:06,  4.44it/s]\u001b[A\n",
            " 75% 76/102 [00:17<00:05,  4.43it/s]\u001b[A\n",
            " 75% 77/102 [00:17<00:05,  4.42it/s]\u001b[A\n",
            " 76% 78/102 [00:17<00:05,  4.42it/s]\u001b[A\n",
            " 77% 79/102 [00:17<00:05,  4.40it/s]\u001b[A\n",
            " 78% 80/102 [00:18<00:04,  4.41it/s]\u001b[A\n",
            " 79% 81/102 [00:18<00:04,  4.41it/s]\u001b[A\n",
            " 80% 82/102 [00:18<00:04,  4.41it/s]\u001b[A\n",
            " 81% 83/102 [00:18<00:04,  4.35it/s]\u001b[A\n",
            " 82% 84/102 [00:18<00:04,  4.37it/s]\u001b[A\n",
            " 83% 85/102 [00:19<00:03,  4.38it/s]\u001b[A\n",
            " 84% 86/102 [00:19<00:03,  4.40it/s]\u001b[A\n",
            " 85% 87/102 [00:19<00:03,  4.33it/s]\u001b[A\n",
            " 86% 88/102 [00:19<00:03,  4.36it/s]\u001b[A\n",
            " 87% 89/102 [00:20<00:02,  4.38it/s]\u001b[A\n",
            " 88% 90/102 [00:20<00:02,  4.39it/s]\u001b[A\n",
            " 89% 91/102 [00:20<00:02,  4.40it/s]\u001b[A\n",
            " 90% 92/102 [00:20<00:02,  4.33it/s]\u001b[A\n",
            " 91% 93/102 [00:21<00:02,  4.34it/s]\u001b[A\n",
            " 92% 94/102 [00:21<00:01,  4.37it/s]\u001b[A\n",
            " 93% 95/102 [00:21<00:01,  4.38it/s]\u001b[A\n",
            " 94% 96/102 [00:21<00:01,  4.39it/s]\u001b[A\n",
            " 95% 97/102 [00:21<00:01,  4.40it/s]\u001b[A\n",
            " 96% 98/102 [00:22<00:00,  4.40it/s]\u001b[A\n",
            " 97% 99/102 [00:22<00:00,  4.41it/s]\u001b[A\n",
            " 98% 100/102 [00:22<00:00,  4.40it/s]\u001b[A\n",
            " 99% 101/102 [00:22<00:00,  4.29it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.16377802193164825, 'eval_runtime': 23.3025, 'eval_samples_per_second': 8.754, 'eval_steps_per_second': 4.377, 'epoch': 4.0}\n",
            " 80% 204/255 [18:36<04:14,  4.99s/it]\n",
            "100% 102/102 [00:23<00:00,  4.31it/s]\u001b[A\n",
            "{'loss': 0.1552, 'grad_norm': 0.25873658061027527, 'learning_rate': 3.14410480349345e-05, 'epoch': 4.31}\n",
            "{'loss': 0.1536, 'grad_norm': 0.25334322452545166, 'learning_rate': 1.3973799126637555e-05, 'epoch': 4.71}\n",
            "100% 255/255 [22:51<00:00,  5.00s/it]\n",
            "  0% 0/102 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/102 [00:00<00:11,  8.88it/s]\u001b[A\n",
            "  3% 3/102 [00:00<00:15,  6.26it/s]\u001b[A\n",
            "  4% 4/102 [00:00<00:18,  5.42it/s]\u001b[A\n",
            "  5% 5/102 [00:00<00:19,  5.04it/s]\u001b[A\n",
            "  6% 6/102 [00:01<00:19,  4.82it/s]\u001b[A\n",
            "  7% 7/102 [00:01<00:20,  4.70it/s]\u001b[A\n",
            "  8% 8/102 [00:01<00:20,  4.62it/s]\u001b[A\n",
            "  9% 9/102 [00:01<00:20,  4.55it/s]\u001b[A\n",
            " 10% 10/102 [00:02<00:20,  4.51it/s]\u001b[A\n",
            " 11% 11/102 [00:02<00:20,  4.49it/s]\u001b[A\n",
            " 12% 12/102 [00:02<00:20,  4.46it/s]\u001b[A\n",
            " 13% 13/102 [00:02<00:20,  4.44it/s]\u001b[A\n",
            " 14% 14/102 [00:02<00:19,  4.43it/s]\u001b[A\n",
            " 15% 15/102 [00:03<00:19,  4.43it/s]\u001b[A\n",
            " 16% 16/102 [00:03<00:19,  4.42it/s]\u001b[A\n",
            " 17% 17/102 [00:03<00:19,  4.33it/s]\u001b[A\n",
            " 18% 18/102 [00:03<00:19,  4.36it/s]\u001b[A\n",
            " 19% 19/102 [00:04<00:18,  4.37it/s]\u001b[A\n",
            " 20% 20/102 [00:04<00:18,  4.39it/s]\u001b[A\n",
            " 21% 21/102 [00:04<00:18,  4.39it/s]\u001b[A\n",
            " 22% 22/102 [00:04<00:18,  4.35it/s]\u001b[A\n",
            " 23% 23/102 [00:04<00:18,  4.38it/s]\u001b[A\n",
            " 24% 24/102 [00:05<00:17,  4.38it/s]\u001b[A\n",
            " 25% 25/102 [00:05<00:17,  4.40it/s]\u001b[A\n",
            " 25% 26/102 [00:05<00:17,  4.40it/s]\u001b[A\n",
            " 26% 27/102 [00:05<00:17,  4.38it/s]\u001b[A\n",
            " 27% 28/102 [00:06<00:16,  4.39it/s]\u001b[A\n",
            " 28% 29/102 [00:06<00:16,  4.40it/s]\u001b[A\n",
            " 29% 30/102 [00:06<00:16,  4.41it/s]\u001b[A\n",
            " 30% 31/102 [00:06<00:16,  4.41it/s]\u001b[A\n",
            " 31% 32/102 [00:07<00:15,  4.41it/s]\u001b[A\n",
            " 32% 33/102 [00:07<00:15,  4.42it/s]\u001b[A\n",
            " 33% 34/102 [00:07<00:15,  4.42it/s]\u001b[A\n",
            " 34% 35/102 [00:07<00:15,  4.40it/s]\u001b[A\n",
            " 35% 36/102 [00:07<00:15,  4.37it/s]\u001b[A\n",
            " 36% 37/102 [00:08<00:14,  4.39it/s]\u001b[A\n",
            " 37% 38/102 [00:08<00:14,  4.39it/s]\u001b[A\n",
            " 38% 39/102 [00:08<00:14,  4.39it/s]\u001b[A\n",
            " 39% 40/102 [00:08<00:14,  4.39it/s]\u001b[A\n",
            " 40% 41/102 [00:09<00:13,  4.39it/s]\u001b[A\n",
            " 41% 42/102 [00:09<00:13,  4.40it/s]\u001b[A\n",
            " 42% 43/102 [00:09<00:13,  4.40it/s]\u001b[A\n",
            " 43% 44/102 [00:09<00:13,  4.39it/s]\u001b[A\n",
            " 44% 45/102 [00:09<00:12,  4.39it/s]\u001b[A\n",
            " 45% 46/102 [00:10<00:12,  4.38it/s]\u001b[A\n",
            " 46% 47/102 [00:10<00:12,  4.39it/s]\u001b[A\n",
            " 47% 48/102 [00:10<00:12,  4.40it/s]\u001b[A\n",
            " 48% 49/102 [00:10<00:12,  4.41it/s]\u001b[A\n",
            " 49% 50/102 [00:11<00:11,  4.41it/s]\u001b[A\n",
            " 50% 51/102 [00:11<00:11,  4.41it/s]\u001b[A\n",
            " 51% 52/102 [00:11<00:11,  4.41it/s]\u001b[A\n",
            " 52% 53/102 [00:11<00:11,  4.33it/s]\u001b[A\n",
            " 53% 54/102 [00:12<00:11,  4.36it/s]\u001b[A\n",
            " 54% 55/102 [00:12<00:10,  4.38it/s]\u001b[A\n",
            " 55% 56/102 [00:12<00:10,  4.39it/s]\u001b[A\n",
            " 56% 57/102 [00:12<00:10,  4.33it/s]\u001b[A\n",
            " 57% 58/102 [00:12<00:10,  4.34it/s]\u001b[A\n",
            " 58% 59/102 [00:13<00:09,  4.36it/s]\u001b[A\n",
            " 59% 60/102 [00:13<00:09,  4.38it/s]\u001b[A\n",
            " 60% 61/102 [00:13<00:09,  4.35it/s]\u001b[A\n",
            " 61% 62/102 [00:13<00:09,  4.30it/s]\u001b[A\n",
            " 62% 63/102 [00:14<00:08,  4.35it/s]\u001b[A\n",
            " 63% 64/102 [00:14<00:08,  4.37it/s]\u001b[A\n",
            " 64% 65/102 [00:14<00:08,  4.39it/s]\u001b[A\n",
            " 65% 66/102 [00:14<00:08,  4.30it/s]\u001b[A\n",
            " 66% 67/102 [00:15<00:08,  4.28it/s]\u001b[A\n",
            " 67% 68/102 [00:15<00:07,  4.33it/s]\u001b[A\n",
            " 68% 69/102 [00:15<00:07,  4.36it/s]\u001b[A\n",
            " 69% 70/102 [00:15<00:07,  4.37it/s]\u001b[A\n",
            " 70% 71/102 [00:15<00:07,  4.39it/s]\u001b[A\n",
            " 71% 72/102 [00:16<00:06,  4.40it/s]\u001b[A\n",
            " 72% 73/102 [00:16<00:06,  4.42it/s]\u001b[A\n",
            " 73% 74/102 [00:16<00:06,  4.42it/s]\u001b[A\n",
            " 74% 75/102 [00:16<00:06,  4.42it/s]\u001b[A\n",
            " 75% 76/102 [00:17<00:05,  4.42it/s]\u001b[A\n",
            " 75% 77/102 [00:17<00:05,  4.41it/s]\u001b[A\n",
            " 76% 78/102 [00:17<00:05,  4.42it/s]\u001b[A\n",
            " 77% 79/102 [00:17<00:05,  4.38it/s]\u001b[A\n",
            " 78% 80/102 [00:17<00:05,  4.39it/s]\u001b[A\n",
            " 79% 81/102 [00:18<00:04,  4.40it/s]\u001b[A\n",
            " 80% 82/102 [00:18<00:04,  4.40it/s]\u001b[A\n",
            " 81% 83/102 [00:18<00:04,  4.37it/s]\u001b[A\n",
            " 82% 84/102 [00:18<00:04,  4.37it/s]\u001b[A\n",
            " 83% 85/102 [00:19<00:03,  4.37it/s]\u001b[A\n",
            " 84% 86/102 [00:19<00:03,  4.33it/s]\u001b[A\n",
            " 85% 87/102 [00:19<00:03,  4.34it/s]\u001b[A\n",
            " 86% 88/102 [00:19<00:03,  4.26it/s]\u001b[A\n",
            " 87% 89/102 [00:20<00:03,  4.29it/s]\u001b[A\n",
            " 88% 90/102 [00:20<00:02,  4.28it/s]\u001b[A\n",
            " 89% 91/102 [00:20<00:02,  4.30it/s]\u001b[A\n",
            " 90% 92/102 [00:20<00:02,  4.32it/s]\u001b[A\n",
            " 91% 93/102 [00:21<00:02,  4.33it/s]\u001b[A\n",
            " 92% 94/102 [00:21<00:01,  4.35it/s]\u001b[A\n",
            " 93% 95/102 [00:21<00:01,  4.37it/s]\u001b[A\n",
            " 94% 96/102 [00:21<00:01,  4.33it/s]\u001b[A\n",
            " 95% 97/102 [00:21<00:01,  4.29it/s]\u001b[A\n",
            " 96% 98/102 [00:22<00:00,  4.31it/s]\u001b[A\n",
            " 97% 99/102 [00:22<00:00,  4.32it/s]\u001b[A\n",
            " 98% 100/102 [00:22<00:00,  4.35it/s]\u001b[A\n",
            " 99% 101/102 [00:22<00:00,  4.35it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.15628507733345032, 'eval_runtime': 23.3136, 'eval_samples_per_second': 8.75, 'eval_steps_per_second': 4.375, 'epoch': 5.0}\n",
            "100% 255/255 [23:14<00:00,  5.00s/it]\n",
            "100% 102/102 [00:23<00:00,  4.37it/s]\u001b[A\n",
            "{'train_runtime': 1395.2269, 'train_samples_per_second': 2.924, 'train_steps_per_second': 0.183, 'train_loss': 0.38702202591241575, 'epoch': 5.0}\n",
            "100% 255/255 [23:15<00:00,  5.47s/it]\n",
            "\n",
            "âœ… Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# æ£€æŸ¥å½“å‰æœ‰å“ªäº›è®­ç»ƒè„šæœ¬\n",
        "!ls -lh *.py\n",
        "\n",
        "# æ£€æŸ¥å·²ä¿å­˜çš„æ¨¡å‹\n",
        "!ls -lh ./outputs_deepseek/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fVQXfPKDRcX",
        "outputId": "91ebd944-55ec-482f-d7a9-feefc4436576"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 12K Oct 23 08:49 train_deepseek_simple.py\n",
            "total 4.0K\n",
            "drwxr-xr-x 4 root root 4.0K Oct 23 09:14 primary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgbXR4irWj_T",
        "outputId": "571a0043-f05d-4ceb-9915-b70471785e80"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Usage:   \n",
            "  pip3 uninstall [options] <package> ...\n",
            "  pip3 uninstall [options] -r <requirements file> ...\n",
            "\n",
            "no such option: -U\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tP9nalCWnyl",
        "outputId": "5bc38d45-3634-48f7-af37-9b73af050b7f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.1)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#section - åˆå¹¶ LoRA æƒé‡\n",
        "print(\"ğŸ”§ å¼€å§‹åˆå¹¶ LoRA æƒé‡...\")\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "# æ¸…ç†æ˜¾å­˜\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "try:\n",
        "    # åŠ è½½ base model (FP16)\n",
        "    print(\"åŠ è½½ base model...\")\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"deepseek-ai/deepseek-llm-7b-base\",\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "        low_cpu_mem_usage=True,\n",
        "    )\n",
        "\n",
        "    # åŠ è½½ LoRA adapter\n",
        "    print(\"åŠ è½½ LoRA adapter...\")\n",
        "    lora_model = PeftModel.from_pretrained(base_model, \"./outputs_deepseek/primary\")\n",
        "\n",
        "    # åˆå¹¶\n",
        "    print(\"åˆå¹¶æƒé‡ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...\")\n",
        "    merged_model = lora_model.merge_and_unload()\n",
        "\n",
        "    # ä¿å­˜\n",
        "    print(\"ä¿å­˜åˆå¹¶åçš„æ¨¡å‹...\")\n",
        "    import os\n",
        "    os.makedirs(\"./outputs_deepseek/primary_merged\", exist_ok=True)\n",
        "    merged_model.save_pretrained(\"./outputs_deepseek/primary_merged\")\n",
        "\n",
        "    # ä¿å­˜ tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-llm-7b-base\", trust_remote_code=True)\n",
        "    tokenizer.save_pretrained(\"./outputs_deepseek/primary_merged\")\n",
        "\n",
        "    print(\"âœ… åˆå¹¶å®Œæˆï¼\")\n",
        "\n",
        "    # æµ‹è¯•åˆå¹¶åçš„æ¨¡å‹\n",
        "    print(\"\\næµ‹è¯•åˆå¹¶åçš„æ¨¡å‹...\")\n",
        "    test_prompt = \"Predict score (1-8) for molecule.\\nSMILES: CCO\\nScore:\"\n",
        "    inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = merged_model.generate(**inputs, max_new_tokens=3)\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        print(f\"æµ‹è¯•è¾“å‡º: {response}\")\n",
        "\n",
        "    # å¤‡ä»½åˆ° Drive\n",
        "    print(\"\\nå¤‡ä»½åˆ° Google Drive...\")\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    !mkdir -p /content/drive/MyDrive/deepseek_backup\n",
        "    !cp -r ./outputs_deepseek/primary_merged /content/drive/MyDrive/deepseek_backup/\n",
        "\n",
        "    print(\"âœ… å…¨éƒ¨å®Œæˆï¼\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ é”™è¯¯: {e}\")\n",
        "    print(\"å¯èƒ½æ˜¯å†…å­˜ä¸è¶³ï¼Œå°è¯•é‡å¯ runtime åå†æ‰§è¡Œ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361,
          "referenced_widgets": [
            "8a9bde019fba4c799bfdeb0d05772a58",
            "eff8fa7b3e724809b6ce987eaa604cd3",
            "83ceea67f7354d288f0e30160c5c96b4",
            "79409b6d7858438db880f3f092fe1d7e",
            "fced1042ae584cfdb25ca917a8e42a95",
            "086cdfc1ee0e4de2b7b75fb918e043d7",
            "6c699725ba7242e8aae835bd093a8454",
            "7ca604546741477eac2271921f790e3f",
            "c1a2bb634f2a4fd79eb5e52864d1575c",
            "ffd2f9a0022a4b8aa6a5dd34b25e3cf8",
            "cc1517dcbfda447093f3639a0e816d41"
          ]
        },
        "id": "t2d57CqNcpUy",
        "outputId": "3e8209aa-8343-41f3-fd43-f5561cfdb406"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ å¼€å§‹åˆå¹¶ LoRA æƒé‡...\n",
            "åŠ è½½ base model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a9bde019fba4c799bfdeb0d05772a58"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "åŠ è½½ LoRA adapter...\n",
            "åˆå¹¶æƒé‡ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...\n",
            "ä¿å­˜åˆå¹¶åçš„æ¨¡å‹...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… åˆå¹¶å®Œæˆï¼\n",
            "\n",
            "æµ‹è¯•åˆå¹¶åçš„æ¨¡å‹...\n",
            "æµ‹è¯•è¾“å‡º: Predict score (1-8) for molecule.\n",
            "SMILES: CCO\n",
            "Score: 1\n",
            "\n",
            "å¤‡ä»½åˆ° Google Drive...\n",
            "Mounted at /content/drive\n",
            "1\n",
            "\n",
            "âœ… å…¨éƒ¨å®Œæˆï¼\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: æŒ‚è½½ Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: å¤åˆ¶ DeepSeek æ¨¡å‹åˆ° Drive\n",
        "!mkdir -p /content/drive/MyDrive/deepseek_backup\n",
        "!cp -r /content/outputs_deepseek/primary /content/drive/MyDrive/deepseek_backup/\n",
        "\n",
        "# éªŒè¯ä¿å­˜æˆåŠŸ\n",
        "!ls -lh /content/drive/MyDrive/deepseek_backup/primary/\n",
        "\n",
        "print(\"âœ… DeepSeek PRIMARY å·²ä¿å­˜åˆ° Google Drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIEgZIt3XL06",
        "outputId": "99bbc9b1-3396-4ba3-d09f-f0dd71389291"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "total 68M\n",
            "-rw------- 1 root root  895 Oct 23 09:18 adapter_config.json\n",
            "-rw------- 1 root root  61M Oct 23 09:18 adapter_model.safetensors\n",
            "drwx------ 2 root root 4.0K Oct 23 09:18 checkpoint-204\n",
            "drwx------ 2 root root 4.0K Oct 23 09:18 checkpoint-255\n",
            "-rw------- 1 root root 5.1K Oct 23 09:18 README.md\n",
            "-rw------- 1 root root  482 Oct 23 09:18 special_tokens_map.json\n",
            "-rw------- 1 root root 3.1K Oct 23 09:18 tokenizer_config.json\n",
            "-rw------- 1 root root 7.2M Oct 23 09:18 tokenizer.json\n",
            "-rw------- 1 root root 5.8K Oct 23 09:18 training_args.bin\n",
            "âœ… DeepSeek PRIMARY å·²ä¿å­˜åˆ° Google Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: é‡æ–°æŒ‚è½½ Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: ä» Drive å¤åˆ¶å›æ¥\n",
        "!mkdir -p /content/outputs_deepseek\n",
        "!cp -r /content/drive/MyDrive/deepseek_backup/primary /content/outputs_deepseek/\n",
        "\n",
        "# éªŒè¯\n",
        "!ls -lh /content/outputs_deepseek/primary/\n",
        "\n",
        "print(\"âœ… DeepSeek PRIMARY å·²ä» Drive æ¢å¤\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xp0L7Z0SXoqy",
        "outputId": "8c1cf49b-6c4f-4cf2-b907-86abdac07653"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "total 68M\n",
            "-rw-r--r-- 1 root root  895 Oct 23 09:20 adapter_config.json\n",
            "-rw-r--r-- 1 root root  61M Oct 23 09:20 adapter_model.safetensors\n",
            "drwxr-xr-x 2 root root 4.0K Oct 23 09:09 checkpoint-204\n",
            "drwxr-xr-x 2 root root 4.0K Oct 23 09:14 checkpoint-255\n",
            "-rw-r--r-- 1 root root 5.1K Oct 23 09:20 README.md\n",
            "-rw-r--r-- 1 root root  482 Oct 23 09:20 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 3.1K Oct 23 09:20 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root 7.2M Oct 23 09:20 tokenizer.json\n",
            "-rw-r--r-- 1 root root 5.8K Oct 23 09:20 training_args.bin\n",
            "âœ… DeepSeek PRIMARY å·²ä» Drive æ¢å¤\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#section 3 - å®Œæ•´ä¿®å¤ç‰ˆ\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM,\n",
        "    Trainer, TrainingArguments, DataCollatorWithPadding, BitsAndBytesConfig\n",
        ")\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ========== è¾…åŠ©å‡½æ•°ï¼ˆä¿æŒä¸å˜ï¼‰==========\n",
        "def seed_everything(seed=42):\n",
        "    import random\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed); os.environ[\"PYTHONHASHSEED\"]=str(seed)\n",
        "    torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
        "\n",
        "def ensure_label_1_8(x):\n",
        "    try: return max(1, min(8, int(round(float(x)))))\n",
        "    except: return 4\n",
        "\n",
        "def within_k(y_true, y_pred, k=1):\n",
        "    return float(np.mean(np.abs(np.array(y_true)-np.array(y_pred)) <= k))\n",
        "\n",
        "def quadratic_weighted_kappa(y_true, y_pred, min_rating=1, max_rating=8):\n",
        "    yt = np.array([ensure_label_1_8(x) for x in y_true], dtype=int)\n",
        "    yp = np.array([ensure_label_1_8(x) for x in y_pred], dtype=int)\n",
        "    m = max_rating - min_rating + 1\n",
        "    O = np.zeros((m,m), float)\n",
        "    for a,b in zip(yt,yp): O[a-min_rating, b-min_rating]+=1\n",
        "    W = np.zeros((m,m), float)\n",
        "    for i in range(m):\n",
        "        for j in range(m):\n",
        "            W[i,j] = ((i-j)**2) / ((m-1)**2)\n",
        "    act = np.sum(O, axis=1); pred = np.sum(O, axis=0)\n",
        "    E = np.outer(act, pred) / np.sum(act)\n",
        "    num = np.sum(W*O); den = np.sum(W*E)\n",
        "    return 1.0 - (num/den if den>0 else 1.0)\n",
        "\n",
        "def evaluate_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        \"MAE\": float(mean_absolute_error(y_true, y_pred)),\n",
        "        \"withinÂ±1\": within_k(y_true, y_pred, 1),\n",
        "        \"QWK\": quadratic_weighted_kappa(y_true, y_pred, 1, 8)\n",
        "    }\n",
        "\n",
        "try:\n",
        "    from rdkit import Chem\n",
        "    from rdkit.Chem.Scaffolds import MurckoScaffold\n",
        "    def get_scaffold(s):\n",
        "        try: return Chem.MolToSmiles(MurckoScaffold.GetScaffoldForMol(Chem.MolFromSmiles(s)))\n",
        "        except: return None\n",
        "except:\n",
        "    def get_scaffold(s): return hash(s) % 10000\n",
        "\n",
        "def scaffold_split(df, smiles_col, test_size=120, val_frac=0.2, seed=42):\n",
        "    df = df.copy()\n",
        "    df[\"scaffold\"] = df[smiles_col].apply(get_scaffold)\n",
        "    df = df[df[\"scaffold\"].notna()].reset_index(drop=True)\n",
        "    groups = df.groupby(\"scaffold\").size().sort_values(ascending=False)\n",
        "    test_scaffolds, cnt = [], 0\n",
        "    for scaf, c in groups.items():\n",
        "        if cnt + c <= test_size * 1.2:\n",
        "            test_scaffolds.append(scaf)\n",
        "            cnt += c\n",
        "            if cnt >= test_size:\n",
        "                break\n",
        "        elif cnt < test_size and test_size - cnt > c * 0.5:\n",
        "            test_scaffolds.append(scaf)\n",
        "            cnt += c\n",
        "            break\n",
        "    test_df = df[df[\"scaffold\"].isin(test_scaffolds)]\n",
        "    remain_df = df[~df[\"scaffold\"].isin(test_scaffolds)]\n",
        "    train_df, val_df = train_test_split(remain_df, test_size=val_frac, random_state=seed)\n",
        "    print(f\"ğŸ“Š Split: Train={len(train_df)} Val={len(val_df)} Test={len(test_df)}\")\n",
        "    return train_df.reset_index(drop=True), val_df.reset_index(drop=True), test_df.reset_index(drop=True)\n",
        "\n",
        "# ========== MolFormerï¼ˆä¿æŒä¸å˜ï¼‰==========\n",
        "class SmilesDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, tokenizer, text_col, label_col, max_length=256):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.tok = tokenizer\n",
        "        self.text_col = text_col\n",
        "        self.label_col = label_col\n",
        "        self.max_length = max_length\n",
        "        labels = [ensure_label_1_8(x) for x in self.df[self.label_col].tolist()]\n",
        "        self.labels = [int(v - 1) for v in labels]\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        s = str(self.df.iloc[idx][self.text_col])\n",
        "        enc = self.tok(s, truncation=True, max_length=self.max_length, padding=False, return_tensors=\"pt\")\n",
        "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "def train_molformer(model_name, train_df, val_df, smiles_col, label_col, out_dir,\n",
        "                    epochs=12, lr=1e-5, bs=8, wd=0.1, seed=42):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    seed_everything(seed)\n",
        "    print(f\"\\nğŸ”§ Training {model_name}\")\n",
        "\n",
        "    tok = AutoTokenizer.from_pretrained(model_name, use_fast=True, trust_remote_code=True)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name, num_labels=8,\n",
        "        torch_dtype=torch.float32,  # âœ… MolFormer ç”¨ FP32\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    train_ds = SmilesDataset(train_df, tok, smiles_col, label_col)\n",
        "    val_ds = SmilesDataset(val_df, tok, smiles_col, label_col)\n",
        "    collator = DataCollatorWithPadding(tokenizer=tok, padding=\"longest\")\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=out_dir, seed=seed, learning_rate=lr, num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=bs, per_device_eval_batch_size=bs, weight_decay=wd,\n",
        "        eval_strategy=\"epoch\", save_strategy=\"epoch\", load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\", greater_is_better=False, warmup_ratio=0.1,\n",
        "        fp16=False,  # âœ… MolFormer å…³é—­ FP16\n",
        "        report_to=\"none\", logging_steps=50, save_total_limit=2,\n",
        "    )\n",
        "\n",
        "    def _metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        preds = np.argmax(logits, axis=-1) + 1\n",
        "        labels = labels + 1\n",
        "        return {\n",
        "            \"mae\": mean_absolute_error(labels, preds),\n",
        "            \"within1\": within_k(labels, preds, 1),\n",
        "            \"qwk\": quadratic_weighted_kappa(labels, preds, 1, 8)\n",
        "        }\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model, args=args, train_dataset=train_ds, eval_dataset=val_ds,\n",
        "        data_collator=collator, tokenizer=tok, compute_metrics=_metrics\n",
        "    )\n",
        "\n",
        "    print(f\"   Training for {epochs} epochs...\")\n",
        "    trainer.train()\n",
        "    trainer.save_model(out_dir)\n",
        "    tok.save_pretrained(out_dir)\n",
        "    print(f\"   âœ… Saved to {out_dir}\")\n",
        "    return model, tok\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_molformer(model, tok, df, smiles_col, label_col, batch_size=64):\n",
        "    ds = SmilesDataset(df, tok, smiles_col, label_col)\n",
        "    collator = DataCollatorWithPadding(tokenizer=tok, padding=\"longest\")\n",
        "    loader = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collator)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device).eval()\n",
        "    all_preds = []\n",
        "    for batch in loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
        "        logits = model(**batch).logits\n",
        "        preds = torch.argmax(logits, dim=-1).cpu().numpy() + 1\n",
        "        all_preds.extend(preds.tolist())\n",
        "    return np.array(all_preds, dtype=int)\n",
        "\n",
        "# DeepSeek é¢„æµ‹\n",
        "@torch.no_grad()\n",
        "def predict_deepseek(model, tokenizer, df, smiles_col):\n",
        "    import re\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    print(f\"\\nğŸ”® Predicting with DeepSeek on {len(df)} samples...\")\n",
        "    for idx, row in df.iterrows():\n",
        "        prompt = f\"Predict score (1-8) for molecule.\\nSMILES: {row[smiles_col]}\\nScore:\"\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=256)\n",
        "        inputs = {k:v.to(device) for k,v in inputs.items()}\n",
        "\n",
        "        try:\n",
        "            outputs = model.generate(**inputs, max_new_tokens=5, temperature=0.1, do_sample=False)\n",
        "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            match = re.search(r'Score:\\s*(\\d)', response)\n",
        "            score = int(match.group(1)) if match else 4\n",
        "            score = max(1, min(8, score))\n",
        "        except:\n",
        "            score = 4\n",
        "\n",
        "        predictions.append(score)\n",
        "        if (idx+1)%30==0:\n",
        "            print(f\"   Progress: {idx+1}/{len(df)}\")\n",
        "\n",
        "    return np.array(predictions)\n",
        "\n",
        "# ========== ä¸»æµç¨‹ ==========\n",
        "print(\"=\"*70)\n",
        "print(\"ğŸš€ MolFormer Agents è®­ç»ƒï¼ˆä¿®å¤ç‰ˆï¼‰\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "seed = 42\n",
        "seed_everything(seed)\n",
        "\n",
        "df = pd.read_excel(\"/content/smiles-data.xlsx\")\n",
        "df = df.dropna(subset=[\"Structure\", \"Score\"])\n",
        "df[\"Score\"] = df[\"Score\"].map(ensure_label_1_8)\n",
        "\n",
        "train_df, val_df, test_df = scaffold_split(df, \"Structure\", 120, 0.2, seed)\n",
        "y_val = val_df[\"Score\"].tolist()\n",
        "y_test = test_df[\"Score\"].tolist()\n",
        "\n",
        "# ========== åŠ è½½ DeepSeekï¼ˆä¿®å¤ï¼‰==========\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"[Step 1] Loading DeepSeek PRIMARY (merged)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "tokenizer_pri = AutoTokenizer.from_pretrained(\n",
        "    \"./outputs_deepseek/primary_merged\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "if not tokenizer_pri.pad_token:\n",
        "    tokenizer_pri.pad_token = tokenizer_pri.eos_token\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_8bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "model_pri = AutoModelForCausalLM.from_pretrained(\n",
        "    \"./outputs_deepseek/primary_merged\",  # âœ… ç›´æ¥åŠ è½½\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "model_pri.eval()\n",
        "print(\"âœ… DeepSeek merged model loaded\")\n",
        "\n",
        "# è¯„ä¼° DeepSeek\n",
        "print(\"\\n[Step 2] Evaluating DeepSeek on test set...\")\n",
        "pri_test_pred = predict_deepseek(model_pri, tokenizer_pri, test_df, \"Structure\")\n",
        "pri_test_m = evaluate_metrics(y_test, pri_test_pred)\n",
        "print(f\"\\nPRIMARY: MAE={pri_test_m['MAE']:.3f} withinÂ±1={pri_test_m['withinÂ±1']:.3f} QWK={pri_test_m['QWK']:.3f}\")\n",
        "\n",
        "# 4. è®­ç»ƒ MolFormer Agent 1\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"[Step 3] Training MolFormer Agent 1\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "m_a1, t_a1 = train_molformer(\n",
        "    \"ibm/MoLFormer-XL-both-10pct\", train_df, val_df,\n",
        "    \"Structure\", \"Score\", \"./outputs_deepseek/agent1\",\n",
        "    epochs=12, lr=1e-5, bs=8, seed=42\n",
        ")\n",
        "\n",
        "a1_test = predict_molformer(m_a1, t_a1, test_df, \"Structure\", \"Score\")\n",
        "a1_test_m = evaluate_metrics(y_test, a1_test)\n",
        "print(f\"\\nAGENT 1: MAE={a1_test_m['MAE']:.3f} withinÂ±1={a1_test_m['withinÂ±1']:.3f} QWK={a1_test_m['QWK']:.3f}\")\n",
        "\n",
        "# 5. è®­ç»ƒ MolFormer Agent 2\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"[Step 4] Training MolFormer Agent 2\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "m_a2, t_a2 = train_molformer(\n",
        "    \"ibm/MoLFormer-XL-both-10pct\", train_df, val_df,\n",
        "    \"Structure\", \"Score\", \"./outputs_deepseek/agent2\",\n",
        "    epochs=12, lr=1e-5, bs=8, seed=142\n",
        ")\n",
        "\n",
        "a2_test = predict_molformer(m_a2, t_a2, test_df, \"Structure\", \"Score\")\n",
        "a2_test_m = evaluate_metrics(y_test, a2_test)\n",
        "print(f\"\\nAGENT 2: MAE={a2_test_m['MAE']:.3f} withinÂ±1={a2_test_m['withinÂ±1']:.3f} QWK={a2_test_m['QWK']:.3f}\")\n",
        "\n",
        "# 6. Verify æœºåˆ¶\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"[Step 5] Verify Mechanism\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "agent_avg = np.round((a1_test + a2_test) / 2).astype(int)\n",
        "agent_avg_m = evaluate_metrics(y_test, agent_avg)\n",
        "print(f\"Agent Average: MAE={agent_avg_m['MAE']:.3f} QWK={agent_avg_m['QWK']:.3f}\")\n",
        "\n",
        "# Verify logic\n",
        "finals = []\n",
        "for i in range(len(pri_test_pred)):\n",
        "    pp = pri_test_pred[i]\n",
        "    aa = agent_avg[i]\n",
        "    mad = abs(a1_test[i] - a2_test[i]) / 2\n",
        "    gap = abs(pp - aa)\n",
        "    accept = (mad <= 1.8) and (gap <= 2)\n",
        "    finals.append(pp if accept else aa)\n",
        "\n",
        "verify_pred = np.array(finals)\n",
        "verify_m = evaluate_metrics(y_test, verify_pred)\n",
        "\n",
        "accept_rate = sum([finals[i] == pri_test_pred[i] for i in range(len(finals))]) / len(finals)\n",
        "\n",
        "print(f\"\\n[VERIFY] MAE={verify_m['MAE']:.3f} withinÂ±1={verify_m['withinÂ±1']:.3f} QWK={verify_m['QWK']:.3f}\")\n",
        "print(f\"Primary acceptance rate: {accept_rate:.1%}\")\n",
        "\n",
        "# 7. ä¿å­˜ç»“æœ\n",
        "results_df = pd.DataFrame({\n",
        "    \"Structure\": test_df[\"Structure\"].values,\n",
        "    \"true\": y_test,\n",
        "    \"primary_deepseek\": pri_test_pred,\n",
        "    \"agent1_molformer\": a1_test,\n",
        "    \"agent2_molformer\": a2_test,\n",
        "    \"agent_avg\": agent_avg,\n",
        "    \"verify_final\": verify_pred\n",
        "})\n",
        "results_df.to_csv(\"./outputs_deepseek/final_predictions.csv\", index=False)\n",
        "\n",
        "# 8. æœ€ç»ˆæ€»ç»“\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ“Š FINAL RESULTS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"PRIMARY (DeepSeek):  MAE={pri_test_m['MAE']:.3f} withinÂ±1={pri_test_m['withinÂ±1']:.3f} QWK={pri_test_m['QWK']:.3f}\")\n",
        "print(f\"Agent 1 (MolFormer): MAE={a1_test_m['MAE']:.3f} withinÂ±1={a1_test_m['withinÂ±1']:.3f} QWK={a1_test_m['QWK']:.3f}\")\n",
        "print(f\"Agent 2 (MolFormer): MAE={a2_test_m['MAE']:.3f} withinÂ±1={a2_test_m['withinÂ±1']:.3f} QWK={a2_test_m['QWK']:.3f}\")\n",
        "print(f\"Agent Average:       MAE={agent_avg_m['MAE']:.3f} withinÂ±1={agent_avg_m['withinÂ±1']:.3f} QWK={agent_avg_m['QWK']:.3f}\")\n",
        "print(f\"VERIFY (Final):      MAE={verify_m['MAE']:.3f} withinÂ±1={verify_m['withinÂ±1']:.3f} QWK={verify_m['QWK']:.3f}\")\n",
        "\n",
        "print(\"\\nğŸ¯ Key Insight:\")\n",
        "if pri_test_m['MAE'] < 2.5:\n",
        "    print(\"   âœ… DeepSeek (general LLM) WORKS on molecular prediction!\")\n",
        "    print(\"   âœ… Demonstrates method generalizability\")\n",
        "else:\n",
        "    print(\"   âš ï¸  DeepSeek struggles, but Verify mechanism recovers performance\")\n",
        "    print(\"   âœ… Multi-agent framework is robust\")\n",
        "\n",
        "print(f\"\\nâœ… All done! Results saved to ./outputs_deepseek/final_predictions.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9df7da4c77ba4141baa61b41f530f384",
            "39325ebcdb8841dab016ec0dfedc0735",
            "0eeb7d6e0b3a48fa955063345d03d92f",
            "3ed85f3d9bf744c8a1bff1a5f440afb8",
            "24a122644d534800a0ea955ed0147aba",
            "4edc64f102fb4efcbdf55af99a72170e",
            "806415b77e4a4387a17c804c454403b3",
            "1595d3cd0e514063b11e3b22ae2a6e08",
            "ef40f1ef1b864f17b2af07a3ae70e31a",
            "e287b9a5be2a40008e8ae6f5b01305fe",
            "1d20e4f9bc8d4ef0bbef2baf21c29d0e"
          ]
        },
        "id": "PW9zgbGHPbE0",
        "outputId": "3261169c-ccad-44be-ee0e-5d0364b08afe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ğŸš€ MolFormer Agents è®­ç»ƒï¼ˆä¿®å¤ç‰ˆï¼‰\n",
            "======================================================================\n",
            "ğŸ“Š Split: Train=816 Val=204 Test=180\n",
            "\n",
            "======================================================================\n",
            "[Step 1] Loading DeepSeek PRIMARY (merged)\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9df7da4c77ba4141baa61b41f530f384"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… DeepSeek merged model loaded\n",
            "\n",
            "[Step 2] Evaluating DeepSeek on test set...\n",
            "\n",
            "ğŸ”® Predicting with DeepSeek on 180 samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Progress: 30/180\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Progress: 60/180\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Progress: 90/180\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Progress: 120/180\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Progress: 150/180\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Progress: 180/180\n",
            "\n",
            "PRIMARY: MAE=3.600 withinÂ±1=0.378 QWK=0.009\n",
            "\n",
            "======================================================================\n",
            "[Step 3] Training MolFormer Agent 1\n",
            "======================================================================\n",
            "\n",
            "ğŸ”§ Training ibm/MoLFormer-XL-both-10pct\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-1735632419.py:134: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Training for 12 epochs...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1224' max='1224' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1224/1224 01:41, Epoch 12/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Mae</th>\n",
              "      <th>Within1</th>\n",
              "      <th>Qwk</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.996200</td>\n",
              "      <td>1.962508</td>\n",
              "      <td>3.637255</td>\n",
              "      <td>0.289216</td>\n",
              "      <td>0.073530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.894700</td>\n",
              "      <td>1.859394</td>\n",
              "      <td>2.083333</td>\n",
              "      <td>0.553922</td>\n",
              "      <td>0.475719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.805000</td>\n",
              "      <td>1.777795</td>\n",
              "      <td>1.720588</td>\n",
              "      <td>0.607843</td>\n",
              "      <td>0.531680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.659900</td>\n",
              "      <td>1.689973</td>\n",
              "      <td>1.352941</td>\n",
              "      <td>0.656863</td>\n",
              "      <td>0.713369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.597400</td>\n",
              "      <td>1.713111</td>\n",
              "      <td>1.377451</td>\n",
              "      <td>0.671569</td>\n",
              "      <td>0.672958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.524500</td>\n",
              "      <td>1.713194</td>\n",
              "      <td>1.509804</td>\n",
              "      <td>0.642157</td>\n",
              "      <td>0.651923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.506300</td>\n",
              "      <td>1.624775</td>\n",
              "      <td>1.352941</td>\n",
              "      <td>0.676471</td>\n",
              "      <td>0.695338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.561000</td>\n",
              "      <td>1.639664</td>\n",
              "      <td>1.303922</td>\n",
              "      <td>0.681373</td>\n",
              "      <td>0.695012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.489300</td>\n",
              "      <td>1.630354</td>\n",
              "      <td>1.308824</td>\n",
              "      <td>0.676471</td>\n",
              "      <td>0.700451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.462500</td>\n",
              "      <td>1.607348</td>\n",
              "      <td>1.289216</td>\n",
              "      <td>0.681373</td>\n",
              "      <td>0.722291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.462400</td>\n",
              "      <td>1.606393</td>\n",
              "      <td>1.264706</td>\n",
              "      <td>0.686275</td>\n",
              "      <td>0.727815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.394400</td>\n",
              "      <td>1.612267</td>\n",
              "      <td>1.313725</td>\n",
              "      <td>0.676471</td>\n",
              "      <td>0.703081</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   âœ… Saved to ./outputs_deepseek/agent1\n",
            "\n",
            "AGENT 1: MAE=2.467 withinÂ±1=0.506 QWK=0.294\n",
            "\n",
            "======================================================================\n",
            "[Step 4] Training MolFormer Agent 2\n",
            "======================================================================\n",
            "\n",
            "ğŸ”§ Training ibm/MoLFormer-XL-both-10pct\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-1735632419.py:134: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Training for 12 epochs...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1224' max='1224' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1224/1224 01:41, Epoch 12/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Mae</th>\n",
              "      <th>Within1</th>\n",
              "      <th>Qwk</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.999300</td>\n",
              "      <td>1.957516</td>\n",
              "      <td>3.524510</td>\n",
              "      <td>0.323529</td>\n",
              "      <td>0.089995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.877400</td>\n",
              "      <td>1.862925</td>\n",
              "      <td>2.313725</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.413582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.806300</td>\n",
              "      <td>1.773563</td>\n",
              "      <td>1.617647</td>\n",
              "      <td>0.612745</td>\n",
              "      <td>0.611322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.710500</td>\n",
              "      <td>1.806200</td>\n",
              "      <td>1.848039</td>\n",
              "      <td>0.553922</td>\n",
              "      <td>0.519934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.668200</td>\n",
              "      <td>1.688096</td>\n",
              "      <td>1.455882</td>\n",
              "      <td>0.647059</td>\n",
              "      <td>0.649591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.540400</td>\n",
              "      <td>1.654921</td>\n",
              "      <td>1.362745</td>\n",
              "      <td>0.671569</td>\n",
              "      <td>0.686772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.491500</td>\n",
              "      <td>1.654220</td>\n",
              "      <td>1.348039</td>\n",
              "      <td>0.671569</td>\n",
              "      <td>0.693902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.485000</td>\n",
              "      <td>1.639932</td>\n",
              "      <td>1.367647</td>\n",
              "      <td>0.656863</td>\n",
              "      <td>0.690603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.465300</td>\n",
              "      <td>1.670623</td>\n",
              "      <td>1.303922</td>\n",
              "      <td>0.656863</td>\n",
              "      <td>0.709393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.516000</td>\n",
              "      <td>1.622977</td>\n",
              "      <td>1.274510</td>\n",
              "      <td>0.661765</td>\n",
              "      <td>0.721331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.436700</td>\n",
              "      <td>1.623411</td>\n",
              "      <td>1.323529</td>\n",
              "      <td>0.671569</td>\n",
              "      <td>0.709512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.482900</td>\n",
              "      <td>1.626517</td>\n",
              "      <td>1.294118</td>\n",
              "      <td>0.681373</td>\n",
              "      <td>0.710175</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   âœ… Saved to ./outputs_deepseek/agent2\n",
            "\n",
            "AGENT 2: MAE=2.306 withinÂ±1=0.517 QWK=0.385\n",
            "\n",
            "======================================================================\n",
            "[Step 5] Verify Mechanism\n",
            "======================================================================\n",
            "Agent Average: MAE=2.350 QWK=0.350\n",
            "\n",
            "[VERIFY] MAE=2.350 withinÂ±1=0.483 QWK=0.355\n",
            "Primary acceptance rate: 20.0%\n",
            "\n",
            "======================================================================\n",
            "ğŸ“Š FINAL RESULTS\n",
            "======================================================================\n",
            "PRIMARY (DeepSeek):  MAE=3.600 withinÂ±1=0.378 QWK=0.009\n",
            "Agent 1 (MolFormer): MAE=2.467 withinÂ±1=0.506 QWK=0.294\n",
            "Agent 2 (MolFormer): MAE=2.306 withinÂ±1=0.517 QWK=0.385\n",
            "Agent Average:       MAE=2.350 withinÂ±1=0.483 QWK=0.350\n",
            "VERIFY (Final):      MAE=2.350 withinÂ±1=0.483 QWK=0.355\n",
            "\n",
            "ğŸ¯ Key Insight:\n",
            "   âš ï¸  DeepSeek struggles, but Verify mechanism recovers performance\n",
            "   âœ… Multi-agent framework is robust\n",
            "\n",
            "âœ… All done! Results saved to ./outputs_deepseek/final_predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#section - ä¿å­˜æ‰€æœ‰ç»“æœå’Œæ¨¡å‹\n",
        "import os\n",
        "import pickle\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ğŸ’¾ ä¿å­˜æ‰€æœ‰ç»“æœå’Œæ¨¡å‹\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# 1. æŒ‚è½½ Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# 2. åˆ›å»ºå¤‡ä»½ç›®å½•\n",
        "backup_dir = \"/content/drive/MyDrive/deepseek_complete_backup\"\n",
        "os.makedirs(backup_dir, exist_ok=True)\n",
        "print(f\"âœ… å¤‡ä»½ç›®å½•: {backup_dir}\")\n",
        "\n",
        "# 3. ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹\n",
        "print(\"\\n[1/6] ä¿å­˜æ¨¡å‹...\")\n",
        "\n",
        "# DeepSeek (merged)\n",
        "if os.path.exists(\"./outputs_deepseek/primary_merged\"):\n",
        "    print(\"  - DeepSeek PRIMARY (merged)\")\n",
        "    !cp -r ./outputs_deepseek/primary_merged {backup_dir}/\n",
        "else:\n",
        "    print(\"  âš ï¸ DeepSeek merged ä¸å­˜åœ¨\")\n",
        "\n",
        "# MolFormer Agent 1\n",
        "if os.path.exists(\"./outputs_deepseek/agent1\"):\n",
        "    print(\"  - MolFormer Agent 1\")\n",
        "    !cp -r ./outputs_deepseek/agent1 {backup_dir}/\n",
        "else:\n",
        "    print(\"  âš ï¸ Agent 1 ä¸å­˜åœ¨\")\n",
        "\n",
        "# MolFormer Agent 2\n",
        "if os.path.exists(\"./outputs_deepseek/agent2\"):\n",
        "    print(\"  - MolFormer Agent 2\")\n",
        "    !cp -r ./outputs_deepseek/agent2 {backup_dir}/\n",
        "else:\n",
        "    print(\"  âš ï¸ Agent 2 ä¸å­˜åœ¨\")\n",
        "\n",
        "# 4. ä¿å­˜æ•°æ®åˆ†å‰²ä¿¡æ¯\n",
        "print(\"\\n[2/6] ä¿å­˜æ•°æ®åˆ†å‰²...\")\n",
        "split_info = {\n",
        "    'train_indices': train_df.index.tolist(),\n",
        "    'val_indices': val_df.index.tolist(),\n",
        "    'test_indices': test_df.index.tolist(),\n",
        "    'train_size': len(train_df),\n",
        "    'val_size': len(val_df),\n",
        "    'test_size': len(test_df),\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "with open(f'{backup_dir}/split_info.pkl', 'wb') as f:\n",
        "    pickle.dump(split_info, f)\n",
        "print(f\"  âœ… ä¿å­˜äº† {len(train_df)} train, {len(val_df)} val, {len(test_df)} test\")\n",
        "\n",
        "# 5. ä¿å­˜é¢„æµ‹ç»“æœ\n",
        "print(\"\\n[3/6] ä¿å­˜é¢„æµ‹ç»“æœ...\")\n",
        "if os.path.exists(\"./outputs_deepseek/final_predictions.csv\"):\n",
        "    !cp ./outputs_deepseek/final_predictions.csv {backup_dir}/\n",
        "    print(\"  âœ… final_predictions.csv\")\n",
        "\n",
        "# 6. ä¿å­˜è¯„ä¼°æŒ‡æ ‡\n",
        "print(\"\\n[4/6] ä¿å­˜è¯„ä¼°æŒ‡æ ‡...\")\n",
        "results_summary = {\n",
        "    'primary': {\n",
        "        'model': 'deepseek-llm-7b-base',\n",
        "        'MAE': pri_test_m['MAE'],\n",
        "        'within_1': pri_test_m['withinÂ±1'],\n",
        "        'QWK': pri_test_m['QWK']\n",
        "    },\n",
        "    'agent1': {\n",
        "        'model': 'MoLFormer-XL',\n",
        "        'MAE': a1_test_m['MAE'],\n",
        "        'within_1': a1_test_m['withinÂ±1'],\n",
        "        'QWK': a1_test_m['QWK']\n",
        "    },\n",
        "    'agent2': {\n",
        "        'model': 'MoLFormer-XL',\n",
        "        'MAE': a2_test_m['MAE'],\n",
        "        'within_1': a2_test_m['withinÂ±1'],\n",
        "        'QWK': a2_test_m['QWK']\n",
        "    },\n",
        "    'agent_average': {\n",
        "        'MAE': agent_avg_m['MAE'],\n",
        "        'within_1': float(np.mean(np.abs(agent_avg - y_test) <= 1)),\n",
        "        'QWK': agent_avg_m['QWK']\n",
        "    },\n",
        "    'verify': {\n",
        "        'MAE': verify_m['MAE'],\n",
        "        'within_1': verify_m['withinÂ±1'],\n",
        "        'QWK': verify_m['QWK'],\n",
        "        'primary_acceptance_rate': accept_rate\n",
        "    }\n",
        "}\n",
        "\n",
        "import json\n",
        "with open(f'{backup_dir}/results_summary.json', 'w') as f:\n",
        "    json.dump(results_summary, f, indent=2)\n",
        "print(\"  âœ… results_summary.json\")\n",
        "\n",
        "# 7. ä¿å­˜è®­ç»ƒé…ç½®\n",
        "print(\"\\n[5/6] ä¿å­˜è®­ç»ƒé…ç½®...\")\n",
        "config = {\n",
        "    'data_path': '/content/smiles-data.xlsx',\n",
        "    'smiles_col': 'Structure',\n",
        "    'label_col': 'Score',\n",
        "    'total_samples': len(df),\n",
        "    'seed': 42,\n",
        "    'deepseek': {\n",
        "        'model': 'deepseek-ai/deepseek-llm-7b-base',\n",
        "        'epochs': 5,\n",
        "        'lr': 2e-4,\n",
        "        'batch_size': 2,\n",
        "    },\n",
        "    'molformer': {\n",
        "        'model': 'ibm/MoLFormer-XL-both-10pct',\n",
        "        'epochs': 12,\n",
        "        'lr': 1e-5,\n",
        "        'batch_size': 8,\n",
        "    },\n",
        "    'verify_mechanism': {\n",
        "        'mad_threshold': 1.8,\n",
        "        'gap_threshold': 2\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(f'{backup_dir}/training_config.json', 'w') as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "print(\"  âœ… training_config.json\")\n",
        "\n",
        "# 8. ä¿å­˜åŸå§‹æ•°æ®çš„å‰¯æœ¬\n",
        "print(\"\\n[6/6] ä¿å­˜åŸå§‹æ•°æ®...\")\n",
        "if os.path.exists('/content/smiles-data.xlsx'):\n",
        "    !cp /content/smiles-data.xlsx {backup_dir}/\n",
        "    print(\"  âœ… smiles-data.xlsx\")\n",
        "\n",
        "# 9. åˆ›å»ºåŠ è½½è„šæœ¬\n",
        "print(\"\\n[Bonus] åˆ›å»ºé‡å¯åŠ è½½è„šæœ¬...\")\n",
        "reload_script = \"\"\"# é‡å¯åè¿è¡Œæ­¤è„šæœ¬\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# 1. æŒ‚è½½ Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. æ¢å¤æ‰€æœ‰æ–‡ä»¶\n",
        "backup_dir = \"/content/drive/MyDrive/deepseek_complete_backup\"\n",
        "os.makedirs('./outputs_deepseek', exist_ok=True)\n",
        "\n",
        "# æ¢å¤æ¨¡å‹\n",
        "!cp -r {backup_dir}/primary_merged ./outputs_deepseek/\n",
        "!cp -r {backup_dir}/agent1 ./outputs_deepseek/\n",
        "!cp -r {backup_dir}/agent2 ./outputs_deepseek/\n",
        "\n",
        "# æ¢å¤æ•°æ®å’Œç»“æœ\n",
        "!cp {backup_dir}/split_info.pkl ./outputs_deepseek/\n",
        "!cp {backup_dir}/final_predictions.csv ./outputs_deepseek/\n",
        "!cp {backup_dir}/results_summary.json ./outputs_deepseek/\n",
        "!cp {backup_dir}/training_config.json ./outputs_deepseek/\n",
        "!cp {backup_dir}/smiles-data.xlsx ./\n",
        "\n",
        "print(\"âœ… æ‰€æœ‰æ–‡ä»¶å·²æ¢å¤ï¼\")\n",
        "\"\"\"\n",
        "\n",
        "with open(f'{backup_dir}/RELOAD_SCRIPT.py', 'w') as f:\n",
        "    f.write(reload_script)\n",
        "print(\"  âœ… RELOAD_SCRIPT.py\")\n",
        "\n",
        "# 10. éªŒè¯ä¿å­˜\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ“‹ ä¿å­˜å†…å®¹éªŒè¯:\")\n",
        "print(\"=\"*70)\n",
        "!ls -lh {backup_dir}/\n",
        "\n",
        "print(\"\\nâœ… å…¨éƒ¨ä¿å­˜å®Œæˆï¼\")\n",
        "print(f\"\\nğŸ“ å¤‡ä»½ä½ç½®: {backup_dir}\")\n",
        "print(\"\\nğŸ’¡ é‡å¯åæ¢å¤æ­¥éª¤:\")\n",
        "print(\"   1. è¿è¡Œ !python /content/drive/MyDrive/deepseek_complete_backup/RELOAD_SCRIPT.py\")\n",
        "print(\"   2. æˆ–è€…æ‰‹åŠ¨å¤åˆ¶æ–‡ä»¶\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8r0VKlXf-Bd",
        "outputId": "9b96a13c-7d35-4eab-9fbc-ad820283dec1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ğŸ’¾ ä¿å­˜æ‰€æœ‰ç»“æœå’Œæ¨¡å‹\n",
            "======================================================================\n",
            "Mounted at /content/drive\n",
            "âœ… å¤‡ä»½ç›®å½•: /content/drive/MyDrive/deepseek_complete_backup\n",
            "\n",
            "[1/6] ä¿å­˜æ¨¡å‹...\n",
            "  - DeepSeek PRIMARY (merged)\n",
            "  - MolFormer Agent 1\n",
            "  - MolFormer Agent 2\n",
            "\n",
            "[2/6] ä¿å­˜æ•°æ®åˆ†å‰²...\n",
            "  âœ… ä¿å­˜äº† 816 train, 204 val, 180 test\n",
            "\n",
            "[3/6] ä¿å­˜é¢„æµ‹ç»“æœ...\n",
            "  âœ… final_predictions.csv\n",
            "\n",
            "[4/6] ä¿å­˜è¯„ä¼°æŒ‡æ ‡...\n",
            "  âœ… results_summary.json\n",
            "\n",
            "[5/6] ä¿å­˜è®­ç»ƒé…ç½®...\n",
            "  âœ… training_config.json\n",
            "\n",
            "[6/6] ä¿å­˜åŸå§‹æ•°æ®...\n",
            "  âœ… smiles-data.xlsx\n",
            "\n",
            "[Bonus] åˆ›å»ºé‡å¯åŠ è½½è„šæœ¬...\n",
            "  âœ… RELOAD_SCRIPT.py\n",
            "\n",
            "======================================================================\n",
            "ğŸ“‹ ä¿å­˜å†…å®¹éªŒè¯:\n",
            "======================================================================\n",
            "total 95K\n",
            "drwx------ 4 root root 4.0K Oct 23 09:57 agent1\n",
            "drwx------ 4 root root 4.0K Oct 23 09:59 agent2\n",
            "-rw------- 1 root root  13K Oct 23 09:59 final_predictions.csv\n",
            "drwx------ 2 root root 4.0K Oct 23 09:57 primary_merged\n",
            "-rw------- 1 root root  747 Oct 23 09:59 RELOAD_SCRIPT.py\n",
            "-rw------- 1 root root  683 Oct 23 09:59 results_summary.json\n",
            "-rw------- 1 root root  64K Oct 23 09:59 smiles-data.xlsx\n",
            "-rw------- 1 root root 3.1K Oct 23 09:59 split_info.pkl\n",
            "-rw------- 1 root root  462 Oct 23 09:59 training_config.json\n",
            "\n",
            "âœ… å…¨éƒ¨ä¿å­˜å®Œæˆï¼\n",
            "\n",
            "ğŸ“ å¤‡ä»½ä½ç½®: /content/drive/MyDrive/deepseek_complete_backup\n",
            "\n",
            "ğŸ’¡ é‡å¯åæ¢å¤æ­¥éª¤:\n",
            "   1. è¿è¡Œ !python /content/drive/MyDrive/deepseek_complete_backup/RELOAD_SCRIPT.py\n",
            "   2. æˆ–è€…æ‰‹åŠ¨å¤åˆ¶æ–‡ä»¶\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# é¢å¤–ä¿å­˜ï¼šç”¨äº Active Learning çš„é¢„æµ‹æ¦‚ç‡\n",
        "print(\"ğŸ’¾ ä¿å­˜ Active Learning æ‰€éœ€æ•°æ®...\")\n",
        "\n",
        "# ä¿å­˜æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„ IDï¼ˆç”¨äºæ’é™¤å·²è®­ç»ƒçš„ï¼‰\n",
        "trained_indices = set(train_df.index.tolist())\n",
        "\n",
        "active_learning_data = {\n",
        "    'trained_indices': list(trained_indices),\n",
        "    'remaining_pool_size': len(df) - len(train_df) - len(val_df) - len(test_df),\n",
        "    'current_train_size': len(train_df),\n",
        "    'target_new_samples': 240,\n",
        "    'models_trained': True,\n",
        "    'stage': 'ready_for_active_learning'\n",
        "}\n",
        "\n",
        "import pickle\n",
        "with open(f'{backup_dir}/active_learning_state.pkl', 'wb') as f:\n",
        "    pickle.dump(active_learning_data, f)\n",
        "\n",
        "print(\"âœ… Active Learning çŠ¶æ€å·²ä¿å­˜\")\n",
        "print(f\"   å½“å‰è®­ç»ƒé›†å¤§å°: {len(train_df)}\")\n",
        "print(f\"   è®¡åˆ’å¢åŠ æ ·æœ¬: 240\")\n",
        "print(f\"   å‰©ä½™å¯ç”¨æ ·æœ¬: {active_learning_data['remaining_pool_size']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lomLHYvVgJwQ",
        "outputId": "99e10cdb-3626-451d-c15b-7a8d7308c2e0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ ä¿å­˜ Active Learning æ‰€éœ€æ•°æ®...\n",
            "âœ… Active Learning çŠ¶æ€å·²ä¿å­˜\n",
            "   å½“å‰è®­ç»ƒé›†å¤§å°: 816\n",
            "   è®¡åˆ’å¢åŠ æ ·æœ¬: 240\n",
            "   å‰©ä½™å¯ç”¨æ ·æœ¬: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#section - DeepSeek åŠ è½½è¯Šæ–­\n",
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ğŸ” DeepSeek åŠ è½½è¯Šæ–­\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# 1. æ£€æŸ¥æ–‡ä»¶\n",
        "print(\"\\n[1] æ£€æŸ¥ä¿å­˜çš„æ–‡ä»¶:\")\n",
        "!ls -lh ./outputs_deepseek/primary/\n",
        "\n",
        "# 2. åŠ è½½æ¨¡å‹\n",
        "print(\"\\n[2] åŠ è½½æ¨¡å‹...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-llm-7b-base\", trust_remote_code=True)\n",
        "if not tokenizer.pad_token:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_8bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"deepseek-ai/deepseek-llm-7b-base\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "lora_model = PeftModel.from_pretrained(base_model, \"./outputs_deepseek/primary\")\n",
        "lora_model.eval()\n",
        "\n",
        "# 3. æ£€æŸ¥ LoRA æ¨¡å—\n",
        "print(\"\\n[3] LoRA æ¨¡å—:\")\n",
        "lora_count = 0\n",
        "for name, module in lora_model.named_modules():\n",
        "    if \"lora\" in name.lower():\n",
        "        lora_count += 1\n",
        "        if lora_count <= 3:  # åªæ‰“å°å‰3ä¸ª\n",
        "            print(f\"  âœ“ {name}\")\n",
        "print(f\"  æ€»å…± {lora_count} ä¸ª LoRA æ¨¡å—\")\n",
        "\n",
        "if lora_count == 0:\n",
        "    print(\"  âŒ æ²¡æœ‰æ‰¾åˆ° LoRA æ¨¡å—ï¼åŠ è½½å¤±è´¥ï¼\")\n",
        "else:\n",
        "    print(\"  âœ… LoRA æ¨¡å—å·²åŠ è½½\")\n",
        "\n",
        "# 4. å¯¹æ¯”é¢„æµ‹\n",
        "print(\"\\n[4] å¯¹æ¯”é¢„æµ‹ (3ä¸ªæµ‹è¯•æ ·æœ¬):\")\n",
        "test_prompts = [\n",
        "    \"Predict score (1-8) for molecule.\\nSMILES: CCO\\nScore:\",\n",
        "    \"Predict score (1-8) for molecule.\\nSMILES: CCCC\\nScore:\",\n",
        "    \"Predict score (1-8) for molecule.\\nSMILES: c1ccccc1\\nScore:\",\n",
        "]\n",
        "\n",
        "for i, prompt in enumerate(test_prompts):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
        "\n",
        "    # Base model\n",
        "    with torch.no_grad():\n",
        "        base_out = base_model.generate(**inputs, max_new_tokens=3, do_sample=False)\n",
        "        base_response = tokenizer.decode(base_out[0], skip_special_tokens=True)\n",
        "        base_score = base_response.split(\"Score:\")[-1].strip()[:1] if \"Score:\" in base_response else \"?\"\n",
        "\n",
        "    # LoRA model\n",
        "    with torch.no_grad():\n",
        "        lora_out = lora_model.generate(**inputs, max_new_tokens=3, do_sample=False)\n",
        "        lora_response = tokenizer.decode(lora_out[0], skip_special_tokens=True)\n",
        "        lora_score = lora_response.split(\"Score:\")[-1].strip()[:1] if \"Score:\" in lora_response else \"?\"\n",
        "\n",
        "    print(f\"\\n  Test {i+1}:\")\n",
        "    print(f\"    Base:  {base_score}\")\n",
        "    print(f\"    LoRA:  {lora_score}\")\n",
        "\n",
        "    if base_score == lora_score:\n",
        "        print(f\"    âš ï¸  ä¸¤ä¸ªæ¨¡å‹é¢„æµ‹ç›¸åŒï¼LoRA å¯èƒ½æ²¡ç”Ÿæ•ˆ\")\n",
        "    else:\n",
        "        print(f\"    âœ…  é¢„æµ‹ä¸åŒï¼ŒLoRA å·²ç”Ÿæ•ˆ\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 900,
          "referenced_widgets": [
            "5dcd7b61a3dc4d6dba0ba860007e49bc",
            "6d71c83c1020416a8f3298630a6f43c6",
            "89dcd720b1324d688875cb7a7caef6b5",
            "b1beb3e1f98449e8a2eb2a5bcfc33bf0",
            "8f286a64b8f74faf942c7c91ee97d56b",
            "be9057ae0e354e5288337ed5e60a3a97",
            "29d9b0c4c9e7432faa48b448bba94255",
            "8b4ad24a90924ac996b788d4bc186553",
            "f008513e653d4164bbb42e79a5094ced",
            "a88b12c498ff483093195d7b605a10fe",
            "f39fbe97c6804b638f96daa3102f5323"
          ]
        },
        "id": "l4bnXwaFcHjb",
        "outputId": "afdff022-29a7-4ea8-a914-ff66a3015d74"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ğŸ” DeepSeek åŠ è½½è¯Šæ–­\n",
            "======================================================================\n",
            "\n",
            "[1] æ£€æŸ¥ä¿å­˜çš„æ–‡ä»¶:\n",
            "total 68M\n",
            "-rw-r--r-- 1 root root  895 Oct 23 09:20 adapter_config.json\n",
            "-rw-r--r-- 1 root root  61M Oct 23 09:20 adapter_model.safetensors\n",
            "drwxr-xr-x 2 root root 4.0K Oct 23 09:09 checkpoint-204\n",
            "drwxr-xr-x 2 root root 4.0K Oct 23 09:14 checkpoint-255\n",
            "-rw-r--r-- 1 root root 5.1K Oct 23 09:20 README.md\n",
            "-rw-r--r-- 1 root root  482 Oct 23 09:20 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 3.1K Oct 23 09:20 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root 7.2M Oct 23 09:20 tokenizer.json\n",
            "-rw-r--r-- 1 root root 5.8K Oct 23 09:20 training_args.bin\n",
            "\n",
            "[2] åŠ è½½æ¨¡å‹...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5dcd7b61a3dc4d6dba0ba860007e49bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[3] LoRA æ¨¡å—:\n",
            "  âœ“ base_model.model.model.layers.0.self_attn.q_proj.lora_dropout\n",
            "  âœ“ base_model.model.model.layers.0.self_attn.q_proj.lora_dropout.default\n",
            "  âœ“ base_model.model.model.layers.0.self_attn.q_proj.lora_A\n",
            "  æ€»å…± 1080 ä¸ª LoRA æ¨¡å—\n",
            "  âœ… LoRA æ¨¡å—å·²åŠ è½½\n",
            "\n",
            "[4] å¯¹æ¯”é¢„æµ‹ (3ä¸ªæµ‹è¯•æ ·æœ¬):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Test 1:\n",
            "    Base:  1\n",
            "    LoRA:  1\n",
            "    âš ï¸  ä¸¤ä¸ªæ¨¡å‹é¢„æµ‹ç›¸åŒï¼LoRA å¯èƒ½æ²¡ç”Ÿæ•ˆ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Test 2:\n",
            "    Base:  1\n",
            "    LoRA:  1\n",
            "    âš ï¸  ä¸¤ä¸ªæ¨¡å‹é¢„æµ‹ç›¸åŒï¼LoRA å¯èƒ½æ²¡ç”Ÿæ•ˆ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Test 3:\n",
            "    Base:  1\n",
            "    LoRA:  1\n",
            "    âš ï¸  ä¸¤ä¸ªæ¨¡å‹é¢„æµ‹ç›¸åŒï¼LoRA å¯èƒ½æ²¡ç”Ÿæ•ˆ\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gQl7Ott4Fje"
      },
      "outputs": [],
      "source": [
        "#section 4\n",
        "# ==== é…ç½® ====\n",
        "REASONING_MODEL = \"meta-llama/Llama-3.1-8b-instruct\"  # å¯æ¢æˆä½ è‡ªå·±çš„HFæŒ‡ä»¤æ¨¡å‹\n",
        "OUT_DIR         = \"/content/outputs_full\"\n",
        "DATA_PATH       = \"/content/smiles-data.xlsx\"\n",
        "SMILES_COL      = \"Structure\"\n",
        "LABEL_COL       = \"Score\"\n",
        "\n",
        "# ==== åŠ è½½ LLAMAï¼ˆHFï¼Œæ— éœ€OpenAIï¼‰ ====\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "import torch, re, os, json, numpy as np, pandas as pd\n",
        "\n",
        "tok_llm = AutoTokenizer.from_pretrained(REASONING_MODEL, use_fast=True)\n",
        "mdl_llm = AutoModelForCausalLM.from_pretrained(\n",
        "    REASONING_MODEL,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
        ")\n",
        "dev_llm = next(mdl_llm.parameters()).device\n",
        "_SCORE_RE = re.compile(r\"\\b([1-8])\\b\")\n",
        "\n",
        "def llm_reason_and_score(smiles, max_new_tokens=128, temperature=0.0):\n",
        "    prompt = (\n",
        "      \"You evaluate mRNA transfection efficiency on a 1â€“8 scale (higher is better).\\n\"\n",
        "      \"Given a molecule SMILES, provide a brief reasoning (1â€“2 sentences), then output the final line:\\n\"\n",
        "      \"Final Score: <single integer 1..8>\\n\"\n",
        "      f\"SMILES: {smiles}\\nReasoning:\\n\"\n",
        "    )\n",
        "    inputs = tok_llm(prompt, return_tensors=\"pt\").to(dev_llm)\n",
        "    gen_cfg = GenerationConfig(max_new_tokens=max_new_tokens, do_sample=(temperature>0),\n",
        "                               temperature=(temperature if temperature>0 else None),\n",
        "                               top_p=1.0, eos_token_id=tok_llm.eos_token_id)\n",
        "    with torch.no_grad():\n",
        "        out = mdl_llm.generate(**inputs, generation_config=gen_cfg)\n",
        "    text = tok_llm.decode(out[0], skip_special_tokens=True)\n",
        "    m = re.search(r\"Final\\s*Score\\s*:\\s*([1-8])\", text, re.I) or _SCORE_RE.search(text)\n",
        "    score = int(m.group(1)) if m else 4\n",
        "    return score, text\n",
        "\n",
        "# ==== è¯»æœ€ä½³ agent ç»„åˆ ====\n",
        "with open(os.path.join(OUT_DIR, \"selection_and_metrics.json\"), \"r\", encoding=\"utf-8\") as f:\n",
        "    sel = json.load(f)\n",
        "best_agents = tuple(sel[\"best_agent_combo_on_val\"][\"agents\"])\n",
        "print(\"Best agents (from val):\", best_agents)\n",
        "\n",
        "# ==== è½½å…¥ä¸¤ä¸ªHFç±» agentæ¨ç†å™¨ ====\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
        "def load_hf_agent(agent_dir):\n",
        "    tok = AutoTokenizer.from_pretrained(agent_dir, use_fast=True)\n",
        "    mdl = AutoModelForSequenceClassification.from_pretrained(agent_dir)\n",
        "    dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    mdl.to(dev).eval()\n",
        "    collator = DataCollatorWithPadding(tokenizer=tok)\n",
        "    def predict(smiles_list, batch_size=64):\n",
        "        class _DS(torch.utils.data.Dataset):\n",
        "            def __init__(self, xs): self.xs=xs\n",
        "            def __len__(self): return len(self.xs)\n",
        "            def __getitem__(self, i):\n",
        "                enc = tok(self.xs[i], truncation=True, max_length=256, return_tensors=\"pt\")\n",
        "                return {k:v.squeeze(0) for k,v in enc.items()}\n",
        "        ds = _DS(smiles_list)\n",
        "        loader = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collator)\n",
        "        preds=[]\n",
        "        with torch.no_grad():\n",
        "            for batch in loader:\n",
        "                batch = {k:v.to(dev) for k,v in batch.items()}\n",
        "                logits = mdl(**batch).logits\n",
        "                yhat = torch.argmax(logits, dim=-1).detach().cpu().numpy()+1\n",
        "                preds += yhat.tolist()\n",
        "        return np.array(preds, dtype=int)\n",
        "    return predict\n",
        "\n",
        "pred_cb = load_hf_agent(os.path.join(OUT_DIR, \"agent_chemberta\"))\n",
        "pred_mf = load_hf_agent(os.path.join(OUT_DIR, \"agent_molformer\"))\n",
        "\n",
        "# ==== Chemprop é¢„æµ‹å™¨ï¼ˆè‹¥å®‰è£…ï¼‰ ====\n",
        "try:\n",
        "    import chemprop\n",
        "    HAVE_CP = True\n",
        "except:\n",
        "    HAVE_CP = False\n",
        "\n",
        "def predict_chemprop_dir(model_dir, smiles_list):\n",
        "    if not HAVE_CP:\n",
        "        return np.array([4]*len(smiles_list), dtype=int)\n",
        "    tmp_csv = os.path.join(model_dir, \"_tmp_pred_llm.csv\")\n",
        "    pd.DataFrame({\"smiles\": smiles_list}).to_csv(tmp_csv, index=False)\n",
        "    args = chemprop.args.PredictArgs().parse_args([\n",
        "        '--test_path', tmp_csv,\n",
        "        '--checkpoint_dir', model_dir,\n",
        "        '--preds_path', os.path.join(model_dir, \"_preds_llm.csv\"),\n",
        "        '--num_workers', '0'\n",
        "    ])\n",
        "    chemprop.train.make_predictions(args)\n",
        "    arr = pd.read_csv(args.preds_path).values.squeeze()\n",
        "    return np.clip(np.round(arr), 1, 8).astype(int)\n",
        "\n",
        "pred_cp = lambda xs: predict_chemprop_dir(os.path.join(OUT_DIR, \"agent_chemprop\"), xs)\n",
        "\n",
        "def predict_by_name(name, smiles_list):\n",
        "    if name == \"ChemBERTa\": return pred_cb(smiles_list)\n",
        "    if name == \"MolFormer\": return pred_mf(smiles_list)\n",
        "    if name == \"Chemprop\":  return pred_cp(smiles_list)\n",
        "    raise ValueError(f\"Unknown agent {name}\")\n",
        "\n",
        "def verify_fuse(llm_score, agent_scores, mad_hi=1.8, gap_hi=2):\n",
        "    a = np.array(agent_scores, float)\n",
        "    a_avg = float(np.mean(a))\n",
        "    a_mad = float(np.median(np.abs(a - np.median(a))))\n",
        "    gap   = abs(llm_score - int(round(a_avg)))\n",
        "    accept = (a_mad <= mad_hi) and (gap <= gap_hi)\n",
        "    final = llm_score if accept else int(round(a_avg))\n",
        "    return int(np.clip(final,1,8)), {\"agent_avg\":a_avg,\"agent_mad\":a_mad,\"gap\":gap,\"accept_llm\":accept}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWVqMuGo4KWN"
      },
      "outputs": [],
      "source": [
        "#section 4.5\n",
        "# è¯»å–è®­ç»ƒè„šæœ¬ç”Ÿæˆçš„ Test120ï¼ˆä¿è¯ä¸ä¸»æµç¨‹ä¸€è‡´ï¼‰\n",
        "test_primary_csv = os.path.join(OUT_DIR, \"test120_primary_predictions.csv\")\n",
        "assert os.path.exists(test_primary_csv), \"è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬ï¼Œç”Ÿæˆæµ‹è¯•åˆ’åˆ†ï¼\"\n",
        "df_test = pd.read_csv(test_primary_csv).rename(columns={\"Structure\":SMILES_COL, \"true\":LABEL_COL})\n",
        "smiles_list = df_test[SMILES_COL].tolist()\n",
        "true_list   = df_test[LABEL_COL].tolist()\n",
        "\n",
        "# è·‘ reasoning + verify\n",
        "rows=[]; final_preds=[]\n",
        "for smi, yt in zip(smiles_list, true_list):\n",
        "    s_llm, reasoning = llm_reason_and_score(smi)\n",
        "    agent_scores = [int(predict_by_name(name, [smi])[0]) for name in best_agents]\n",
        "    s_final, diag = verify_fuse(s_llm, agent_scores)\n",
        "    final_preds.append(s_final)\n",
        "    rows.append({\n",
        "        \"smiles\": smi, \"true\": yt, \"llm_score\": s_llm,\n",
        "        **{f\"agent_{name}\": agent_scores[i] for i,name in enumerate(best_agents)},\n",
        "        \"agent_avg\": diag[\"agent_avg\"], \"agent_mad\": diag[\"agent_mad\"], \"gap\": diag[\"gap\"],\n",
        "        \"accept_llm\": diag[\"accept_llm\"], \"final_score\": s_final,\n",
        "        \"reasoning\": reasoning[:800]\n",
        "    })\n",
        "\n",
        "# è¯„ä¼°ä¸ä¿å­˜\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "def within1(y,p): return float(np.mean(np.abs(np.array(y)-np.array(p))<=1))\n",
        "def qwk(y,p,lo=1,hi=8):\n",
        "    y=np.array(y,int); p=np.array(p,int); m=hi-lo+1\n",
        "    O=np.zeros((m,m))\n",
        "    for a,b in zip(y,p): O[a-lo,b-lo]+=1\n",
        "    W=np.zeros((m,m))\n",
        "    for i in range(m):\n",
        "        for j in range(m): W[i,j]=((i-j)**2)/((m-1)**2)\n",
        "    act=O.sum(1); pred=O.sum(0); E=np.outer(act,pred)/act.sum()\n",
        "    num=(W*O).sum(); den=(W*E).sum(); return 1.0-(num/den if den>0 else 1.0)\n",
        "\n",
        "metrics = {\n",
        "    \"MAE\": float(mean_absolute_error(true_list, final_preds)),\n",
        "    \"withinÂ±1\": within1(true_list, final_preds),\n",
        "    \"QWK\": qwk(true_list, final_preds)\n",
        "}\n",
        "print(\"Reasoning+Verify (Test120):\", metrics)\n",
        "\n",
        "out_csv = os.path.join(OUT_DIR, \"test120_reasoning_verify_llama.csv\")\n",
        "out_json= os.path.join(OUT_DIR, \"test120_reasoning_verify_llama_metrics.json\")\n",
        "pd.DataFrame(rows).to_csv(out_csv, index=False)\n",
        "import json\n",
        "with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(metrics, f, ensure_ascii=False, indent=2)\n",
        "print(\"Saved:\", out_csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PimIgz3h4OgO"
      },
      "outputs": [],
      "source": [
        "# =======================\n",
        "# section 4.6 â€” Active Learning Loop (æ•´åˆåˆ° Section 4 åç»­)\n",
        "# ä¾èµ–ï¼šå·²å®Œæˆ Section 2/3 çš„è®­ç»ƒè„šæœ¬ & Section 4/4.5 çš„åŠ è½½\n",
        "# ä½¿ç”¨åŒä¸€ OUT_DIR / BEST_AGENTS / pred_* é¢„æµ‹å™¨\n",
        "# =======================\n",
        "import os, json, numpy as np, pandas as pd\n",
        "from copy import deepcopy\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import torch\n",
        "\n",
        "# ---- ä¿®å¤ï¼šç»§æ‰¿Section 4çš„best_agents ----\n",
        "BEST_AGENTS = best_agents  # ä»Section 4ç»§æ‰¿\n",
        "print(\"ğŸ”„ Active Learningä½¿ç”¨çš„æœ€ä½³Agentç»„åˆ:\", BEST_AGENTS)\n",
        "\n",
        "# ---- åŸºç¡€å·¥å…· ----\n",
        "def ensure_label_1_8(x):\n",
        "    try:\n",
        "        v = int(round(float(x)))\n",
        "    except:\n",
        "        v = 4\n",
        "    return max(1, min(8, v))\n",
        "\n",
        "def within_k(y_true, y_pred, k=1):\n",
        "    yt = np.asarray([ensure_label_1_8(x) for x in y_true], dtype=int)\n",
        "    yp = np.asarray([ensure_label_1_8(x) for x in y_pred], dtype=int)\n",
        "    return float(np.mean(np.abs(yt-yp) <= k))\n",
        "\n",
        "def quadratic_weighted_kappa(y_true, y_pred, lo=1, hi=8):\n",
        "    y = np.asarray([ensure_label_1_8(x) for x in y_true], dtype=int)\n",
        "    p = np.asarray([ensure_label_1_8(x) for x in y_pred], dtype=int)\n",
        "    m = hi - lo + 1\n",
        "    O = np.zeros((m,m), float)\n",
        "    for a,b in zip(y,p): O[a-lo,b-lo]+=1\n",
        "    W = np.zeros((m,m), float)\n",
        "    for i in range(m):\n",
        "        for j in range(m): W[i,j] = ((i-j)**2)/((m-1)**2)\n",
        "    act = O.sum(1); prd = O.sum(0)\n",
        "    E = np.outer(act, prd) / (act.sum() if act.sum()>0 else 1)\n",
        "    num = (W*O).sum(); den = (W*E).sum() if (W*E).sum()>0 else 1.0\n",
        "    return 1.0 - num/den\n",
        "\n",
        "def evaluate_model_df(df_eval, preds, label_col=LABEL_COL):\n",
        "    y_true = df_eval[label_col].tolist()\n",
        "    return {\n",
        "        \"MAE\": float(mean_absolute_error(y_true, preds)),\n",
        "        \"withinÂ±1\": within_k(y_true, preds, 1),\n",
        "        \"QWK\": quadratic_weighted_kappa(y_true, preds, 1, 8)\n",
        "    }\n",
        "\n",
        "# ---- è½½å…¥ä¸»æ¨¡å‹ï¼ˆç”¨äºå¢é‡è®­ç»ƒï¼‰ ----\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "\n",
        "PRIMARY_DIR = os.path.join(OUT_DIR, \"primary\")\n",
        "assert os.path.isdir(PRIMARY_DIR), \"æœªæ‰¾åˆ°ä¸»æ¨¡å‹ç›®å½•ï¼Œè¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬ï¼ˆsection 3ï¼‰ç”Ÿæˆ OUT_DIR/primaryã€‚\"\n",
        "\n",
        "primary_tok = AutoTokenizer.from_pretrained(PRIMARY_DIR, use_fast=True)\n",
        "primary_model = AutoModelForSequenceClassification.from_pretrained(PRIMARY_DIR)\n",
        "\n",
        "# æ¦‚ç‡ä¸é¢„æµ‹ï¼ˆç”¨äºç†µï¼‰\n",
        "def predict_hf_probs(model, tokenizer, smiles_list, max_len=256, batch_size=64):\n",
        "    class _DS(torch.utils.data.Dataset):\n",
        "        def __init__(self, xs): self.xs=xs\n",
        "        def __len__(self): return len(self.xs)\n",
        "        def __getitem__(self, i):\n",
        "            enc = tokenizer(self.xs[i], truncation=True, max_length=max_len, return_tensors=\"pt\")\n",
        "            return {k:v.squeeze(0) for k,v in enc.items()}\n",
        "    collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "    ds = _DS(smiles_list)\n",
        "    loader = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collator)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device); model.eval()\n",
        "    probs_all=[]; preds=[]\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = {k:v.to(device) for k,v in batch.items()}\n",
        "            logits = model(**batch).logits\n",
        "            probs  = torch.softmax(logits, dim=-1).detach().cpu().numpy()\n",
        "            yhat   = np.argmax(probs, axis=-1)+1\n",
        "            probs_all.append(probs); preds += yhat.tolist()\n",
        "    return np.vstack(probs_all), np.array(preds, int)\n",
        "\n",
        "# ---- ä¸ç¡®å®šæ€§åº¦é‡ï¼šä¸»æ¨¡å‹ç†µ + Agentåˆ†æ­§ + å¤šæ ·æ€§æƒ©ç½š ----\n",
        "from math import log\n",
        "\n",
        "def entropy_row(probs_row, eps=1e-9):\n",
        "    p = np.clip(probs_row, eps, 1.0)\n",
        "    p = p / p.sum()\n",
        "    return -np.sum(p * np.log(p))\n",
        "\n",
        "def agent_disagreement(agent_preds_int):\n",
        "    arr = np.array(agent_preds_int, dtype=float)\n",
        "    std = float(np.std(arr))\n",
        "    q75, q25 = np.percentile(arr, [75,25])\n",
        "    iqr = float(q75 - q25)\n",
        "    return 0.7*std + 0.3*iqr\n",
        "\n",
        "# RDKit å¤šæ ·æ€§ï¼ˆè‹¥ä¸å¯ç”¨åˆ™é€€åŒ–ä¸º0æƒ©ç½šï¼‰\n",
        "try:\n",
        "    from rdkit import Chem\n",
        "    from rdkit.Chem import AllChem, DataStructs\n",
        "    HAVE_RDKIT_AL = True\n",
        "except Exception:\n",
        "    HAVE_RDKIT_AL = False\n",
        "\n",
        "_fp_cache = {}\n",
        "def morgan_fp(smi, radius=2, nbits=2048):\n",
        "    if not HAVE_RDKIT_AL: return None\n",
        "    if smi in _fp_cache: return _fp_cache[smi]\n",
        "    m = Chem.MolFromSmiles(smi)\n",
        "    if m is None:\n",
        "        _fp_cache[smi] = None; return None\n",
        "    fp = AllChem.GetMorganFingerprintAsBitVect(m, radius, nBits=nbits)\n",
        "    _fp_cache[smi] = fp\n",
        "    return fp\n",
        "\n",
        "def diversity_penalty(smi, labeled_smiles_set, radius=2, nbits=2048):\n",
        "    if not HAVE_RDKIT_AL or len(labeled_smiles_set)==0:\n",
        "        return 0.0\n",
        "    fp = morgan_fp(smi, radius, nbits)\n",
        "    if fp is None: return 0.5\n",
        "    sims=[]\n",
        "    for ls in labeled_smiles_set:\n",
        "        fp2 = morgan_fp(ls, radius, nbits)\n",
        "        if fp2 is None: continue\n",
        "        sims.append(DataStructs.TanimotoSimilarity(fp, fp2))\n",
        "    if len(sims)==0: return 0.0\n",
        "    return float(max(sims))  # ä¸å·²æ ‡æ³¨é›†ä¸­æœ€åƒçš„ç›¸ä¼¼åº¦ï¼ˆ0~1ï¼‰ï¼Œè¶Šå¤§æƒ©ç½šè¶Šå¤§\n",
        "\n",
        "def select_uncertain_samples(primary_model, primary_tok, agent_names, pool_df, labeled_df,\n",
        "                             budget=50, w_entropy=0.6, w_disagree=0.3, w_div=0.1):\n",
        "    smiles = pool_df[SMILES_COL].astype(str).tolist()\n",
        "\n",
        "    # ä¸»æ¨¡å‹æ¦‚ç‡ä¸ç†µ\n",
        "    pri_probs, _ = predict_hf_probs(primary_model, primary_tok, smiles)\n",
        "    pri_entropy = np.apply_along_axis(entropy_row, 1, pri_probs)\n",
        "\n",
        "    # Agent åˆ†æ­§ï¼ˆä½¿ç”¨ä½ åœ¨ Section 4 å·²åŠ è½½å¥½çš„é¢„æµ‹å™¨ï¼‰\n",
        "    agent_all_preds = []\n",
        "    for name in agent_names:\n",
        "        if name == \"ChemBERTa\":\n",
        "            _, preds = predict_hf_probs(\n",
        "                AutoModelForSequenceClassification.from_pretrained(os.path.join(OUT_DIR, \"agent_chemberta\")).eval().to(next(primary_model.parameters()).device),\n",
        "                AutoTokenizer.from_pretrained(os.path.join(OUT_DIR, \"agent_chemberta\"), use_fast=True),\n",
        "                smiles\n",
        "            )\n",
        "        elif name == \"MolFormer\":\n",
        "            _, preds = predict_hf_probs(\n",
        "                AutoModelForSequenceClassification.from_pretrained(os.path.join(OUT_DIR, \"agent_molformer\")).eval().to(next(primary_model.parameters()).device),\n",
        "                AutoTokenizer.from_pretrained(os.path.join(OUT_DIR, \"agent_molformer\"), use_fast=True),\n",
        "                smiles\n",
        "            )\n",
        "        elif name == \"Chemprop\":\n",
        "            try:\n",
        "                import chemprop\n",
        "                tmp = os.path.join(OUT_DIR, \"agent_chemprop\", \"_al_tmp.csv\")\n",
        "                pd.DataFrame({\"smiles\": smiles}).to_csv(tmp, index=False)\n",
        "                args = chemprop.args.PredictArgs().parse_args([\n",
        "                    '--test_path', tmp,\n",
        "                    '--checkpoint_dir', os.path.join(OUT_DIR, \"agent_chemprop\"),\n",
        "                    '--preds_path', os.path.join(OUT_DIR, \"agent_chemprop\", \"_al_preds.csv\"),\n",
        "                    '--num_workers', '0'\n",
        "                ])\n",
        "                chemprop.train.make_predictions(args)\n",
        "                arr = pd.read_csv(args.preds_path).values.squeeze()\n",
        "                preds = np.clip(np.round(arr), 1, 8).astype(int)\n",
        "            except Exception:\n",
        "                preds = np.full(len(smiles), 4, dtype=int)\n",
        "        else:\n",
        "            continue\n",
        "        agent_all_preds.append(preds)\n",
        "    if len(agent_all_preds)==0:\n",
        "        disagree = np.zeros(len(smiles))\n",
        "    else:\n",
        "        agent_all_preds = np.vstack(agent_all_preds)\n",
        "        disagree = np.apply_along_axis(agent_disagreement, 0, agent_all_preds)\n",
        "\n",
        "    # å¤šæ ·æ€§æƒ©ç½š\n",
        "    labeled_smiles_set = set(labeled_df[SMILES_COL].astype(str).tolist()) if labeled_df is not None else set()\n",
        "    div_pen = np.array([diversity_penalty(s, labeled_smiles_set) for s in smiles])\n",
        "\n",
        "    # ç»„åˆä¸ç¡®å®šæ€§\n",
        "    score = w_entropy*pri_entropy + w_disagree*disagree + w_div*div_pen\n",
        "    idx_sorted = np.argsort(-score)[:budget]\n",
        "    return pool_df.iloc[idx_sorted].copy(), score[idx_sorted]\n",
        "\n",
        "# ---- è·å–æ ‡ç­¾ï¼šçœŸå®å®éªŒ / æ¨¡æ‹Ÿï¼ˆå ä½ï¼‰ ----\n",
        "def get_experimental_labels(df_selected, mode=\"simulate\"):\n",
        "    \"\"\"\n",
        "    mode='real'ï¼šè¯·å°†å®éªŒç»“æœå†™å…¥ df_selected[LABEL_COL] å return df_selected\n",
        "    mode='simulate'ï¼šç”¨éªŒè¯é›†æŒ‘å‡ºçš„ BEST_AGENTS çš„å‡å€¼ä½œä¸ºå ä½æ ‡ç­¾ï¼ˆä¾¿äºè·‘é€šæµç¨‹ï¼‰\n",
        "    \"\"\"\n",
        "    if mode == \"real\":\n",
        "        # TODO: å®éªŒè¿”å›åæŠŠçœŸå®æ ‡ç­¾å†™å…¥ df_selected[LABEL_COL]\n",
        "        raise NotImplementedError(\"è¯·æŠŠå®éªŒæ ‡ç­¾å†™å…¥ df_selected['Score'] åè¿”å›ã€‚\")\n",
        "    else:\n",
        "        preds_list = []\n",
        "        smiles = df_selected[SMILES_COL].astype(str).tolist()\n",
        "        for name in BEST_AGENTS:\n",
        "            if name == \"ChemBERTa\":\n",
        "                _, p = predict_hf_probs(\n",
        "                    AutoModelForSequenceClassification.from_pretrained(os.path.join(OUT_DIR, \"agent_chemberta\")).eval().to(next(primary_model.parameters()).device),\n",
        "                    AutoTokenizer.from_pretrained(os.path.join(OUT_DIR, \"agent_chemberta\"), use_fast=True),\n",
        "                    smiles\n",
        "                )\n",
        "            elif name == \"MolFormer\":\n",
        "                _, p = predict_hf_probs(\n",
        "                    AutoModelForSequenceClassification.from_pretrained(os.path.join(OUT_DIR, \"agent_molformer\")).eval().to(next(primary_model.parameters()).device),\n",
        "                    AutoTokenizer.from_pretrained(os.path.join(OUT_DIR, \"agent_molformer\"), use_fast=True),\n",
        "                    smiles\n",
        "                )\n",
        "            elif name == \"Chemprop\":\n",
        "                try:\n",
        "                    import chemprop\n",
        "                    tmp = os.path.join(OUT_DIR, \"agent_chemprop\", \"_al_tmp2.csv\")\n",
        "                    pd.DataFrame({\"smiles\": smiles}).to_csv(tmp, index=False)\n",
        "                    args = chemprop.args.PredictArgs().parse_args([\n",
        "                        '--test_path', tmp,\n",
        "                        '--checkpoint_dir', os.path.join(OUT_DIR, \"agent_chemprop\"),\n",
        "                        '--preds_path', os.path.join(OUT_DIR, \"agent_chemprop\", \"_al_preds2.csv\"),\n",
        "                        '--num_workers', '0'\n",
        "                    ])\n",
        "                    chemprop.train.make_predictions(args)\n",
        "                    arr = pd.read_csv(args.preds_path).values.squeeze()\n",
        "                    p = np.clip(np.round(arr), 1, 8).astype(int)\n",
        "                except Exception:\n",
        "                    p = np.full(len(smiles), 4, dtype=int)\n",
        "            preds_list.append(p)\n",
        "        pseudo = np.clip(np.round(np.mean(np.vstack(preds_list), axis=0)), 1, 8).astype(int)\n",
        "        out = df_selected.copy()\n",
        "        out[LABEL_COL] = pseudo\n",
        "        return out\n",
        "\n",
        "# ---- ä¸»æ¨¡å‹å¢é‡è®­ç»ƒ ----\n",
        "class _SmilesDS(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, tok, text_col, label_col, max_length=256):\n",
        "        self.df = df.reset_index(drop=True); self.tok=tok\n",
        "        self.text_col=text_col; self.label_col=label_col; self.max_length=max_length\n",
        "        labels = [ensure_label_1_8(x) for x in self.df[self.label_col].tolist()]\n",
        "        self.labels = [int(v-1) for v in labels]\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        s = str(self.df.iloc[idx][self.text_col])\n",
        "        enc = self.tok(s, truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
        "        item = {k:v.squeeze(0) for k,v in enc.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "def incremental_training_primary(primary_dir, primary_model, primary_tok, add_df, val_df,\n",
        "                                 smiles_col=SMILES_COL, label_col=LABEL_COL,\n",
        "                                 epochs=1, lr=2e-5, bs=16, wd=0.01, seed=42, save_suffix=\"al\"):\n",
        "    os.makedirs(primary_dir, exist_ok=True)\n",
        "    train_ds = _SmilesDS(add_df, primary_tok, smiles_col, label_col)\n",
        "    val_ds   = _SmilesDS(val_df,   primary_tok, smiles_col, label_col)\n",
        "    collator = DataCollatorWithPadding(tokenizer=primary_tok)\n",
        "    args = TrainingArguments(\n",
        "        output_dir=primary_dir, seed=seed,\n",
        "        learning_rate=lr, num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=bs, per_device_eval_batch_size=bs,\n",
        "        weight_decay=wd, eval=\"epoch\", save_strategy=\"no\",\n",
        "        warmup_ratio=0.1, bf16=torch.cuda.is_available(), report_to=\"none\", logging_steps=50\n",
        "    )\n",
        "    def _metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        preds = np.argmax(logits, axis=-1) + 1\n",
        "        labels = labels + 1\n",
        "        return {\n",
        "            \"mae\": mean_absolute_error(labels, preds),\n",
        "            \"within1\": within_k(labels, preds, 1),\n",
        "            \"qwk\": quadratic_weighted_kappa(labels, preds, 1, 8)\n",
        "        }\n",
        "    from transformers import default_data_collator\n",
        "\n",
        "    trainer = Trainer(model=primary_model, args=args, train_dataset=train_ds, eval_dataset=val_ds,\n",
        "                      data_collator=collator, tokenizer=primary_tok, compute_metrics=_metrics)\n",
        "    trainer.train()\n",
        "    primary_model.save_pretrained(os.path.join(primary_dir, f\"{save_suffix}_checkpoint\"))\n",
        "    primary_tok.save_pretrained(os.path.join(primary_dir, f\"{save_suffix}_checkpoint\"))\n",
        "    return primary_model\n",
        "\n",
        "# ---- å‡†å¤‡ Pool / Labeled / Val / Test ----\n",
        "# å…¨é‡æ•°æ®ï¼ˆä¸ä½ çš„ DATA_PATH ä¸€è‡´ï¼‰\n",
        "if DATA_PATH.lower().endswith(\".xlsx\"):\n",
        "    df_all = pd.read_excel(DATA_PATH)\n",
        "else:\n",
        "    df_all = pd.read_csv(DATA_PATH)\n",
        "df_all = df_all.dropna(subset=[SMILES_COL, LABEL_COL]).reset_index(drop=True)\n",
        "df_all[LABEL_COL] = df_all[LABEL_COL].map(ensure_label_1_8)\n",
        "\n",
        "# Test120ï¼ˆæ²¿ç”¨è®­ç»ƒè„šæœ¬ç”Ÿæˆçš„åˆ’åˆ†ï¼‰\n",
        "test_csv = os.path.join(OUT_DIR, \"test120_primary_predictions.csv\")\n",
        "assert os.path.exists(test_csv), \"è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬ï¼ˆSection 3ï¼‰ï¼Œç”Ÿæˆ Test120 æ–‡ä»¶ã€‚\"\n",
        "df_test = pd.read_csv(test_csv).rename(columns={\"Structure\":SMILES_COL, \"true\":LABEL_COL})\n",
        "df_test[LABEL_COL] = df_test[LABEL_COL].map(ensure_label_1_8)\n",
        "\n",
        "# Pool = All - Test120ï¼› åˆå§‹åŒ–å·²æ ‡æ³¨ï¼ˆæ¼”ç¤ºç”¨ï¼‰ä» Pool éšæœºæŠ½å– N_INIT æ¡\n",
        "test_smiles_set = set(df_test[SMILES_COL].astype(str).tolist())\n",
        "df_pool = df_all[~df_all[SMILES_COL].astype(str).isin(test_smiles_set)].copy().reset_index(drop=True)\n",
        "\n",
        "N_INIT = 100  # è¿™é‡Œä»…æ¼”ç¤ºã€‚å®æˆ˜ä¸­è¯·ç”¨ä½ çœŸå®çš„ train+val ä½œä¸º labeled_df åˆå§‹é›†\n",
        "rng = np.random.default_rng(42)\n",
        "init_idx = rng.choice(len(df_pool), size=min(N_INIT, len(df_pool)), replace=False)\n",
        "labeled_df = df_pool.iloc[init_idx].copy().reset_index(drop=True)\n",
        "pool_df    = df_pool.drop(index=init_idx).reset_index(drop=True)\n",
        "\n",
        "# ç”¨ Test120 çš„å‰40æ¡ä½œä¸º AL çš„éªŒè¯é›†ï¼ˆä»…æ¼”ç¤ºï¼›å®æˆ˜å»ºè®®å›ºå®šä½ è®­ç»ƒæ—¶çš„ val é›†ï¼‰\n",
        "df_val_for_al = df_test.iloc[:40].copy().reset_index(drop=True)\n",
        "\n",
        "print(f\"[AL init] Pool={len(pool_df)}  Labeled={len(labeled_df)}  Val_for_AL={len(df_val_for_al)}  Test120={len(df_test)}\")\n",
        "\n",
        "# ---- ä¸»å¾ªç¯ï¼šActive Learning ----\n",
        "def active_learning_cycle(primary_model, primary_tok, pool_df, labeled_df, val_df, test_df,\n",
        "                          initial_budget=50, cycles=3, mode=\"simulate\"):\n",
        "    model = primary_model\n",
        "    history=[]\n",
        "    for c in range(cycles):\n",
        "        print(f\"\\nğŸ”„ Active Learning Cycle {c+1}/{cycles}\")\n",
        "        # 1) é€‰æ ·\n",
        "        sel_df, _ = select_uncertain_samples(model, primary_tok, BEST_AGENTS, pool_df, labeled_df,\n",
        "                                             budget=initial_budget, w_entropy=0.6, w_disagree=0.3, w_div=0.1)\n",
        "        # 2) æ ‡ç­¾ï¼ˆçœŸå®/æ¨¡æ‹Ÿï¼‰\n",
        "        new_labeled = get_experimental_labels(sel_df, mode=mode)\n",
        "        # 3) åˆå¹¶å·²æ ‡æ³¨\n",
        "        labeled_df = pd.concat([labeled_df, new_labeled], axis=0).drop_duplicates(subset=[SMILES_COL]).reset_index(drop=True)\n",
        "        # 4) ä»æ± ä¸­ç§»é™¤\n",
        "        used = set(new_labeled[SMILES_COL].astype(str).tolist())\n",
        "        pool_df = pool_df[~pool_df[SMILES_COL].astype(str).isin(used)].reset_index(drop=True)\n",
        "        # 5) å¢é‡è®­ç»ƒä¸»æ¨¡å‹\n",
        "        model = incremental_training_primary(\n",
        "            PRIMARY_DIR, model, primary_tok, new_labeled, val_df,\n",
        "            epochs=1, lr=2e-5, bs=16, wd=0.01, seed=42, save_suffix=f\"al_c{c+1}\"\n",
        "        )\n",
        "        # 6) Test120 è¯„ä¼°\n",
        "        probs_test, preds_test = predict_hf_probs(model, primary_tok, test_df[SMILES_COL].astype(str).tolist())\n",
        "        met = evaluate_model_df(test_df, preds_test, label_col=LABEL_COL)\n",
        "        history.append({\"cycle\": c+1, \"test_metrics\": met, \"pool_remaining\": len(pool_df), \"labeled_total\": len(labeled_df)})\n",
        "        print(f\"[Cycle {c+1}] Test120: MAE={met['MAE']:.3f} Â±1={met['withinÂ±1']:.3f} QWK={met['QWK']:.3f} | Pool={len(pool_df)} Labeled={len(labeled_df)}\")\n",
        "        if len(pool_df)==0:\n",
        "            print(\"Pool is empty. Stop.\"); break\n",
        "    return model, labeled_df, pool_df, history\n",
        "\n",
        "# === è¿è¡Œä¸€ä¸ªç¤ºä¾‹ï¼ˆå¯æ ¹æ®éœ€è¦è°ƒæ•´ cycles / initial_budget / modeï¼‰ ===\n",
        "improved_model, labeled_df_final, pool_df_final, al_history = active_learning_cycle(\n",
        "    primary_model, primary_tok, pool_df, labeled_df, df_val_for_al, df_test,\n",
        "    initial_budget=50, cycles=3, mode=\"simulate\"   # å®éªŒå›å¡«æ—¶æ”¹ä¸º mode=\"real\"\n",
        ")\n",
        "\n",
        "# ä¿å­˜å†å²\n",
        "al_hist_path = os.path.join(OUT_DIR, \"active_learning_history.csv\")\n",
        "pd.DataFrame(al_history).to_csv(al_hist_path, index=False)\n",
        "print(\"âœ… Active Learning finished. History saved to:\", al_hist_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbIbbxWM4WiJ"
      },
      "outputs": [],
      "source": [
        "#section 5\n",
        "import os, json, pandas as pd, numpy as np\n",
        "\n",
        "OUT_DIR = \"/content/outputs_full\"\n",
        "sel_path = os.path.join(OUT_DIR, \"selection_and_metrics.json\")\n",
        "assert os.path.exists(sel_path), \"è¯·å…ˆå®Œæˆä¸»è®­ç»ƒè„šæœ¬è¿è¡Œã€‚\"\n",
        "\n",
        "with open(sel_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    summary = json.load(f)\n",
        "\n",
        "print(\"== ä¸»æ¨¡å‹ Test ==\")\n",
        "print(summary[\"primary_test\"])\n",
        "\n",
        "print(\"\\n== å„ Agent å•ç‹¬ Test ==\")\n",
        "for k,v in summary[\"agents_test\"].items():\n",
        "    print(f\"{k:12s} -> MAE={v['MAE']:.3f}  Â±1={v['withinÂ±1']:.3f}  QWK={v['QWK']:.3f}\")\n",
        "\n",
        "print(\"\\n== éªŒè¯é›†é€‰æ‹©çš„æœ€ä½³ç»„åˆ ==\")\n",
        "print(summary[\"best_agent_combo_on_val\"])\n",
        "\n",
        "print(\"\\n== Testï¼ˆä¸»æ¨¡å‹/å„Agent/æœ€ä½³ç»„åˆ/å…¨éƒ¨ç»„åˆï¼‰ ==\")\n",
        "for k,v in summary[\"test_reports\"].items():\n",
        "    print(f\"{k:25s} -> MAE={v['MAE']:.3f}  Â±1={v['withinÂ±1']:.3f}  QWK={v['QWK']:.3f}\")\n",
        "\n",
        "print(\"\\n== Verify åçš„æœ€ç»ˆæŒ‡æ ‡ï¼ˆä¸»æ¨¡å‹+é—¨å«ï¼‰ ==\")\n",
        "print(summary[\"verify_test_metrics\"])\n",
        "\n",
        "# é€æ ·æœ¬å¯¹ç…§ï¼ˆçœ‹çœ‹å“ªäº›è¢«å›é€€ï¼‰\n",
        "df_primary = pd.read_csv(os.path.join(OUT_DIR, \"test120_primary_predictions.csv\"))\n",
        "df_agents  = pd.read_csv(os.path.join(OUT_DIR, \"test120_agent_predictions.csv\"))\n",
        "df_verify  = pd.read_csv(os.path.join(OUT_DIR, \"test120_verify_predictions.csv\"))\n",
        "\n",
        "df = df_primary.merge(df_agents, on=[\"Structure\",\"true\"], how=\"left\") \\\n",
        "               .merge(df_verify[[\"smiles\",\"final_score\",\"agent_mad\",\"gap\",\"accept_primary\"]],\n",
        "                      left_on=\"Structure\", right_on=\"smiles\", how=\"left\") \\\n",
        "               .drop(columns=[\"smiles\"])\n",
        "df.rename(columns={\"final_score\":\"verify_final\"}, inplace=True)\n",
        "\n",
        "accept_rate = float(np.mean(df[\"accept_primary\"]))\n",
        "print(f\"\\nVerify æ¥å—ä¸»æ¨¡å‹æ¯”ä¾‹: {accept_rate:.2%}\")\n",
        "\n",
        "print(\"\\nè¢«å›é€€çš„å‰10æ¡æ ·æœ¬ï¼š\")\n",
        "df[df[\"accept_primary\"]==False].head(10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2c75135ee25e4f6fa6358367d882a1ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_295dceb1342a4d779fec1a15099d642d"
          }
        },
        "4858747f49fc4bb9a6853200aeb92029": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_febff21a75964dae82d280a11d476ba4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ca418843d9e04cdd95dc3a0287331ca2",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "4c8cbaa72e134b36b6e1659ec170005b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_f4e05deca27041649e5dd2ff86db5793",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7039b5defbf940eb841d0dcd9bff7804",
            "value": ""
          }
        },
        "226c574474884f6fb76c9646b5891142": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_ab9a4e85c32c48b891786408082e57d8",
            "style": "IPY_MODEL_6000573289ec44608924259cc3145544",
            "value": true
          }
        },
        "6438ac8f67fe45fbb0a2f726f030c161": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_97b4a21c28164860bcc34ea3256b8ea4",
            "style": "IPY_MODEL_60585ca2b5a94e56a79981fe04ec05d1",
            "tooltip": ""
          }
        },
        "7b5ce8a38c374e7e984b703159b95acd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b081dd3e63d45e393fbee38702a7e71",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2a2b0a6cbf14450e9a978e164d0da1a1",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "295dceb1342a4d779fec1a15099d642d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "febff21a75964dae82d280a11d476ba4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca418843d9e04cdd95dc3a0287331ca2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4e05deca27041649e5dd2ff86db5793": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7039b5defbf940eb841d0dcd9bff7804": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab9a4e85c32c48b891786408082e57d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6000573289ec44608924259cc3145544": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97b4a21c28164860bcc34ea3256b8ea4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60585ca2b5a94e56a79981fe04ec05d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "7b081dd3e63d45e393fbee38702a7e71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a2b0a6cbf14450e9a978e164d0da1a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f00fc252101471b8e50df78dfe351be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fff8707fda3a464d8ef0d3f7ba0346f7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f103bc5dcf214fa5b2a30d0ac1762bf3",
            "value": "Connecting..."
          }
        },
        "fff8707fda3a464d8ef0d3f7ba0346f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f103bc5dcf214fa5b2a30d0ac1762bf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a9bde019fba4c799bfdeb0d05772a58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eff8fa7b3e724809b6ce987eaa604cd3",
              "IPY_MODEL_83ceea67f7354d288f0e30160c5c96b4",
              "IPY_MODEL_79409b6d7858438db880f3f092fe1d7e"
            ],
            "layout": "IPY_MODEL_fced1042ae584cfdb25ca917a8e42a95"
          }
        },
        "eff8fa7b3e724809b6ce987eaa604cd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_086cdfc1ee0e4de2b7b75fb918e043d7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6c699725ba7242e8aae835bd093a8454",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "83ceea67f7354d288f0e30160c5c96b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ca604546741477eac2271921f790e3f",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c1a2bb634f2a4fd79eb5e52864d1575c",
            "value": 2
          }
        },
        "79409b6d7858438db880f3f092fe1d7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffd2f9a0022a4b8aa6a5dd34b25e3cf8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_cc1517dcbfda447093f3639a0e816d41",
            "value": "â€‡2/2â€‡[00:04&lt;00:00,â€‡â€‡1.90s/it]"
          }
        },
        "fced1042ae584cfdb25ca917a8e42a95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "086cdfc1ee0e4de2b7b75fb918e043d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c699725ba7242e8aae835bd093a8454": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ca604546741477eac2271921f790e3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1a2bb634f2a4fd79eb5e52864d1575c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ffd2f9a0022a4b8aa6a5dd34b25e3cf8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc1517dcbfda447093f3639a0e816d41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9df7da4c77ba4141baa61b41f530f384": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_39325ebcdb8841dab016ec0dfedc0735",
              "IPY_MODEL_0eeb7d6e0b3a48fa955063345d03d92f",
              "IPY_MODEL_3ed85f3d9bf744c8a1bff1a5f440afb8"
            ],
            "layout": "IPY_MODEL_24a122644d534800a0ea955ed0147aba"
          }
        },
        "39325ebcdb8841dab016ec0dfedc0735": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4edc64f102fb4efcbdf55af99a72170e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_806415b77e4a4387a17c804c454403b3",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "0eeb7d6e0b3a48fa955063345d03d92f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1595d3cd0e514063b11e3b22ae2a6e08",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ef40f1ef1b864f17b2af07a3ae70e31a",
            "value": 3
          }
        },
        "3ed85f3d9bf744c8a1bff1a5f440afb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e287b9a5be2a40008e8ae6f5b01305fe",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1d20e4f9bc8d4ef0bbef2baf21c29d0e",
            "value": "â€‡3/3â€‡[00:16&lt;00:00,â€‡â€‡5.25s/it]"
          }
        },
        "24a122644d534800a0ea955ed0147aba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4edc64f102fb4efcbdf55af99a72170e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "806415b77e4a4387a17c804c454403b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1595d3cd0e514063b11e3b22ae2a6e08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef40f1ef1b864f17b2af07a3ae70e31a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e287b9a5be2a40008e8ae6f5b01305fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d20e4f9bc8d4ef0bbef2baf21c29d0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5dcd7b61a3dc4d6dba0ba860007e49bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6d71c83c1020416a8f3298630a6f43c6",
              "IPY_MODEL_89dcd720b1324d688875cb7a7caef6b5",
              "IPY_MODEL_b1beb3e1f98449e8a2eb2a5bcfc33bf0"
            ],
            "layout": "IPY_MODEL_8f286a64b8f74faf942c7c91ee97d56b"
          }
        },
        "6d71c83c1020416a8f3298630a6f43c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be9057ae0e354e5288337ed5e60a3a97",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_29d9b0c4c9e7432faa48b448bba94255",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "89dcd720b1324d688875cb7a7caef6b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b4ad24a90924ac996b788d4bc186553",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f008513e653d4164bbb42e79a5094ced",
            "value": 2
          }
        },
        "b1beb3e1f98449e8a2eb2a5bcfc33bf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a88b12c498ff483093195d7b605a10fe",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f39fbe97c6804b638f96daa3102f5323",
            "value": "â€‡2/2â€‡[00:15&lt;00:00,â€‡â€‡7.13s/it]"
          }
        },
        "8f286a64b8f74faf942c7c91ee97d56b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be9057ae0e354e5288337ed5e60a3a97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29d9b0c4c9e7432faa48b448bba94255": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b4ad24a90924ac996b788d4bc186553": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f008513e653d4164bbb42e79a5094ced": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a88b12c498ff483093195d7b605a10fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f39fbe97c6804b638f96daa3102f5323": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}